[{"content":"A booming, Etsy-based online retailer called BananAppeal.co was in need of a landing page, and I wanted to help them out. So, in less than one day, I assembled a simple (but very sleek) landing page for their business.\nYou can view the page here: bananappeal.co\nVideo Sorry, your browser doesn't support embedded videos.  Performance/Accessibility/Best Practices/SEO Using Lighthouse, the website meets almost 100% across every web-based metric:\nThe only metric it falls behind on is the recommendation to serve next-gen images instead of the more universally compatible jpg/png images. I opted not to go through that exercise for this project.\nAttributions  Pink notebook Figurines iPhone near mac FontAwesome Bootstrap AOS  Technical discussion The site is very simple; despite this portfolio item being categorized as \u0026ldquo;full stack\u0026rdquo;, this is purely frontend, uses almost no JavaScript whatsoever. I am leveraging the latest Bootstrap 5 for the layout and components.\nAnimations The AOS library really makes a difference, though. The animations are extremely easy to implement:\n\u0026lt;div class=\u0026#34;col-12\u0026#34; data-aos-offset=\u0026#34;0\u0026#34; data-aos-delay=\u0026#34;25\u0026#34; data-aos-duration=\u0026#34;1000\u0026#34; data-aos=\u0026#34;fade-up\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; It should be fairly obvious that aos.js simply looks at every element in the document, and applies a standard set of CSS animations based on the value of data-xyz values. In HTML, anything with data- is intended to be used as an arbitrary data store, which is quite powerful, and allows for a lot of awesome libraries to shine (with minimal effort) like aos.js.\nBy mixing up the offsets and delays, I was able to create a very polished-looking set of scrolling animations, which can be seen as you scroll down the website.\nBiggest challenge The biggest challenge was that the awesome parallax effect is actually disabled on iOS (and probably Android) devices due to the fact that it (apparently) uses a lot of resources to render. When developing the site on a regular computer, it looks amazing - parallax included - but, on the mobile device, the background\u0026rsquo;s color is the only thing we see. This was a bit of a bummer.\nA lot of posts on StackOverflow throw some hack-ish solutions for this, but I always opt to go for the simplest and most robust solution. In this case, using CSS media queries to simply render the main logo without parallax on mobile devices, and with parallax on other devices. Here\u0026rsquo;s how to do it:\n/* gets applied first, and allows parallax to work on all regular devices */ .bg-bananappeal { background-image: url(\u0026#39;images/bananappeal.co.svg\u0026#39;); background-size: contain; background-repeat: no-repeat; background-color: #FFDE59; background-attachment: fixed; background-position: center; } /* gets applied first, and hides the logo on all regular devices*/ .logo-mobile-only { display: none; } /* only on mobile will the parallax effect be unset: */ /* https://stackoverflow.com/a/47517612 */ @media (pointer: coarse) { .bg-bananappeal { background-image: unset !important; background-size: unset !important; background-repeat: unset !important; background-attachment: unset !important; background-position: unset !important; } /* only on mobile will the logo be actually visible: */ .logo-mobile-only { display: block; } } Efficiency/Optimizations In general, the footprint of this project is fairly small. I didn\u0026rsquo;t want to minify or compress anything since I can generally rely on Cloudflare\u0026rsquo;s proxy to do caching and minifying. Work smarter, not harder - if there was actually a scenario where I had to minify and compress, I would do it.\nSource Code You can view the source code here on GitHub.\nHosting The site is hosted via GitHub pages, and proxied behind Cloudflare, which automatically uses TLS connections and manages them independently, so we don\u0026rsquo;t have to deal with LetsEncrypt or other certificate management services. Very convenient.\nWhere the results of this project went BananAppeal.co is gaining traction, and I\u0026rsquo;m happy to have been a part of their journey. Best of luck!\n","permalink":"https://charlesmknox.com/portfolio/fullstack/bananappealco/","summary":"A booming, Etsy-based online retailer called BananAppeal.co was in need of a landing page, and I wanted to help them out. So, in less than one day, I assembled a simple (but very sleek) landing page for their business.\nYou can view the page here: bananappeal.co\nVideo Sorry, your browser doesn't support embedded videos.  Performance/Accessibility/Best Practices/SEO Using Lighthouse, the website meets almost 100% across every web-based metric:\nThe only metric it falls behind on is the recommendation to serve next-gen images instead of the more universally compatible jpg/png images.","title":"BananAppeal.co: Small Business Landing Page"},{"content":" Firefox Containers Helper v0.0.14 is out now, as of 3/17/2021 - previously v0.0.10, released back in November 2020, a few awesome features have been added!\nCheck out the extension View the extension here! You can also view the source code here on GitHub.\nThank you to the community What makes this release of Firefox Containers Helper special is that it has received some awesome community engagement! I really never thought that it would gain enough attention or usage from people around the world for people to create GitHub issues, pull requests, and become supporters\nNew features added  Dark and Light mode - Bootstrap dark mode which respects your system theme preference  Note: On my Ubuntu system, changing Firefox\u0026rsquo;s theme wasn\u0026rsquo;t enough, I had to change my entire theme from a dark theme to a light theme to switch the preference   Select Mode - Allows you to precisely select only a couple results from the list by first enabling the mode and then pressing Ctrl+Click, or multiple by pressing Ctrl+Shift+Click Configurable Keyboard Shortcut - Allows you to change the keyboard shortcut for showing the extension popup window, which is by default Alt+Shift+D. This fixes #4 Container Quick-Add - Allows you to quickly add a new container based on what you have typed into the filter text box. Defaults to a circle icon and the toolbar color  Multi-selection mode live example Sorry, your browser doesn't support embedded videos.  Keyboard shortcuts are now configurable Here\u0026rsquo;s a screenshot of the new options page for the keyboard shortcut configuration:\nChanges straight from the changelog v0.0.12-v0.0.14  fix #11, the indexing for multi-select was using the total number of containers instead of the filtered number of containers ðŸ˜° re-add \u0026ldquo;+\u0026rdquo; button for containers since it\u0026rsquo;s actually fine added stern warning to the readme about NEVER disabling or deleting all of your container extensions in Firefox, or else they\u0026rsquo;ll get completely reset  v0.0.11  dark mode and light mode added in #8  Thanks to https://github.com/KerfuffleV2 for this contribution!   configurable keyboard shortcut added (see #4) adds Select Mode, as requested by the project\u0026rsquo;s first supporter!  resolves #6   fix #9\u0026hellip; no longer drops all casing to lowercase when using \u0026ldquo;replace in name\u0026rdquo; mode, sorry about that! \u0026ldquo;replace in name\u0026rdquo; used to be case-insensitive, but I believe it\u0026rsquo;s better to have case-sensitive replacing  Please file an issue and talk with me if you disagree or believe there should be a better approach! Thank you! ðŸ™‚   added a small \u0026ldquo;Donate\u0026rdquo; link to my personal site with my personal avatar, I appreciate your support everyone ðŸ™‚ added a simple \u0026ldquo;+\u0026rdquo; button to quickly add a new container based on the user\u0026rsquo;s current input, this was requested here  Final note I hope you enjoy the extension! It\u0026rsquo;s been steadily growing in popularity and I am happy to see people using it.\n","permalink":"https://charlesmknox.com/tech/software/updates/firefox-containers-helper-v0.0.14/","summary":"Firefox Containers Helper v0.0.14 is out now, as of 3/17/2021 - previously v0.0.10, released back in November 2020, a few awesome features have been added!\nCheck out the extension View the extension here! You can also view the source code here on GitHub.\nThank you to the community What makes this release of Firefox Containers Helper special is that it has received some awesome community engagement! I really never thought that it would gain enough attention or usage from people around the world for people to create GitHub issues, pull requests, and become supporters","title":"Firefox Containers Helper Extension v0.0.14 Released"},{"content":"ðŸ’‰ Introduction A friend of mine, Tynick, and I crossed paths on a project he started - he wanted to get a COVID vaccine by doing a volunteer shift. The two websites that offer volunteering signups occasionally had openings, but they were always taken almost immediately.\nSo, he had the brilliant idea to use some clever Python to automatically check the website, and notify him with Slack or Twilio whenever there\u0026rsquo;s a signup available.\nCheckout the \u0026ldquo;Using Python to get an early COVID vaccine\u0026rdquo; post on tynick.com for all the juicy details.\nTynick\u0026rsquo;s original work is an excellent, minimalist solution that contains only the bare minimum needed to run the project and get the job done. While we were chatting about this bot, I found that I wanted to run it too, but I didn\u0026rsquo;t want to have both of our bots running simultaneously, and I didn\u0026rsquo;t want to get my IP address on a blocklist. Additionally, I didn\u0026rsquo;t want to run it on Heroku.\nSo, I dockerized it, pointed it to a local Tor proxy (included in this repo), added some environment variable configuration, and wrapped everything together in a docker compose file. See the architecture section for details. My hope is that the audience reading Tynick\u0026rsquo;s original post might want to take things a little further and see a version of the app that has a bit extra added on top.\nI hope readers find this iteration on the project useful! Tynick and I had a blast with this project and we were also happy to help friends get vaccinated.\nLink to the source code, and the fork The source code from Tynick is here: https://github.com/tynick/covid-vaccine-bot\nAnd the source code for my fork, which is what we\u0026rsquo;re talking about for the rest of this writeup, is here: https://github.com/charles-m-knox/covid-vaccine-bot\nYou can see an open PR between the two repositories here - this PR will remain open for illustrative purposes: https://github.com/tynick/covid-vaccine-bot/pull/1\nGetting Started This setup requires Docker, Docker compose, and optionally, GNU Make.\n If you don\u0026rsquo;t have Docker, visit get.docker.com and follow the install steps in the comments at the top of the file  Helpful advice if you can\u0026rsquo;t find it: It will be something like curl \u0026lt;script\u0026gt; followed by sh get-docker.sh   If you don\u0026rsquo;t have Docker compose, visit the official installation instructions page for compose. If you don\u0026rsquo;t have GNU Make, or are not sure how to check if you have it, simply run the command make -h in your terminal. You should see some help text print out in your terminal.  If you definitely don\u0026rsquo;t have it, installation steps vary from platform, but if you\u0026rsquo;re on a system like Ubuntu you can run sudo apt install make - see here.    First, make a copy of .env.sample.env into the file .env:\ncp .env.sample.env .env Modify the values in it to correspond to your Slack and Twilio configuration - if you\u0026rsquo;re not using one of the services such as Twilio, you can leave its values blank:\nSLACK_WEBHOOK_URL=https://hooks.slack.com/services/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 TWILIO_ACCOUNT_SID=xyz TWILIO_AUTH_TOKEN=xyz TWILIO_TARGET_NUMBER=+15555555555 TWILIO_SOURCE_NUMBER=+15555555555  Note: Do not use quotes. Also, that\u0026rsquo;s not a real Slack webhook, even though it looks like one.\n Next, if you have make installed, run the commands:\nmake pull # this will fail to pull the vaccine-bot image, that\u0026#39;s expected - building it next make build-image make run make logs # optional: will follow logs If you don\u0026rsquo;t have make, that\u0026rsquo;s OK - the bash commands that it runs are contained in the Makefile and you can still do everything as follows:\ndocker-compose pull # this will fail to pull the vaccine-bot image, that\u0026#39;s expected - building it next docker build -t covid-vaccine-bot:latest . docker-compose up -d docker-compose logs -f # follows logs Making sense of the log output Take a look at the below logs, which are captured from running docker-compose logs -f (or make logs if you\u0026rsquo;re using the Makefile). This is fairly normal. The following is happening, in order:\n The COVID vaccine bot is running, and is trying to make a request to the volunteer website. The COVID vaccine bot\u0026rsquo;s requests are failing (hence why we see exceptions occurring), because the tor-privoxy service hasn\u0026rsquo;t started yet. The tor-privoxy service finally starts up, as indicated by the fact that we see messages coming from the service. The COVID vaccine bot starts to succeed, because the Tor connection has been established successfully.   Note: Tor will always take a few moments to start up. This is because it is creating and securing a circuit (as well as a couple other things), which is a set of 3 networked devices in the Tor network that the tor-privoxy service will make its requests through. You can see this with the Tor browser on startup too.\n covid-vaccine-bot | During handling of the above exception, another exception occurred: covid-vaccine-bot | covid-vaccine-bot | Traceback (most recent call last): covid-vaccine-bot | File \u0026quot;/src/covid-vaccine-bot.py\u0026quot;, line 66, in \u0026lt;module\u0026gt; covid-vaccine-bot | vaccine_site_info = get_site(site_url) covid-vaccine-bot | File \u0026quot;/src/covid-vaccine-bot.py\u0026quot;, line 58, in get_site covid-vaccine-bot | response = requests.get(url, headers=headers) covid-vaccine-bot | File \u0026quot;/usr/local/lib/python3.9/site-packages/requests/api.py\u0026quot;, line 76, in get covid-vaccine-bot | return request('get', url, params=params, **kwargs) covid-vaccine-bot | File \u0026quot;/usr/local/lib/python3.9/site-packages/requests/api.py\u0026quot;, line 61, in request covid-vaccine-bot | return session.request(method=method, url=url, **kwargs) covid-vaccine-bot | File \u0026quot;/usr/local/lib/python3.9/site-packages/requests/sessions.py\u0026quot;, line 542, in request covid-vaccine-bot | resp = self.send(prep, **send_kwargs) covid-vaccine-bot | File \u0026quot;/usr/local/lib/python3.9/site-packages/requests/sessions.py\u0026quot;, line 655, in send covid-vaccine-bot | r = adapter.send(request, **kwargs) covid-vaccine-bot | File \u0026quot;/usr/local/lib/python3.9/site-packages/requests/adapters.py\u0026quot;, line 510, in send covid-vaccine-bot | raise ProxyError(e, request=request) covid-vaccine-bot | requests.exceptions.ProxyError: HTTPSConnectionPool(host='www.handsonphoenix.org', port=443): Max retries exceeded with url: /opportunity/a0N1J00000QGgU1UAL (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 503 Forwarding failure'))) tor-privoxy | tor-privoxy | * /proc is already mounted tor-privoxy | * /run/openrc: creating directory tor-privoxy | * /run/lock: creating directory tor-privoxy | * /run/lock: correcting owner tor-privoxy | * Starting privoxy ... [ ok ] tor-privoxy | * Setting up Tor control relay ... [ ok ] tor-privoxy | * /var/lib/tor: correcting owner tor-privoxy | * /run/tor: creating directory tor-privoxy | * /run/tor: correcting owner tor-privoxy | * Starting tor ... [ ok ] tor-privoxy | OpenRC 0.39.2.7e1d41d609 is starting up Linux 5.8.0-44-generic (x86_64) [DOCKER] tor-privoxy | tor-privoxy | * Caching service dependencies ... [ ok ] covid-vaccine-bot | getting site... covid-vaccine-bot | site retrieved... covid-vaccine-bot | nothing was available covid-vaccine-bot | getting site... covid-vaccine-bot | site retrieved... covid-vaccine-bot | There is an opening at State Farm Stadium. Check https://www.handsonphoenix.org/opportunity/a0N1J00000NW4CHUA1 for more info. Architecture The vaccine bot relies on the tor-privoxy service being alive in order to make its outbound requests, including Slack and Twilio.\n Note: It\u0026rsquo;s posible Twilio may reject requests from behind the Tor network. I haven\u0026rsquo;t tested it, but Slack appears to work.\n Why use Tor? What is it? If you aren\u0026rsquo;t familiar with Tor, you should take a moment to read and understand what it is. I\u0026rsquo;ll try and summarize.\nFor all intents and purposes, Tor is a couple things:\n It\u0026rsquo;s a couple thousand systems run by regular people like you and I, or run by corporations or other entities. It\u0026rsquo;s a protocol for taking your network requests and bouncing them around those thousands of systems across the world, to form a \u0026ldquo;circuit\u0026rdquo;. It is also a means to access hidden services, or Onion sites, which are long URLs that differ from a typical xyz.com website: 3g2upl4pq6kufc4m.onion - this is the DuckDuckGo hidden service, for example.  Most hidden services have much longer names. Hidden services are more or less randomly generated names (there is a lot of cryptography behind it). Some hidden services, like Facebook, used brute force to randomly generate hidden service names until they got one they like: facebookcorewwwi.onion    Tor helps you become a bit more \u0026ldquo;anonymous\u0026rdquo; on the web. Normally, when you visit a webpage, the traffic first gets relayed by your internet service provider (such as Cox or Comcast), out to the world wide web, and to the destination. When you visit Tor, the traffic does technically also go through your ISP, but there are so many layers of cryptography and network trickery at work that they have no realistic way of knowing what you\u0026rsquo;re doing, and effectively, they cannot monitor you.\nHowever, simply proxying requests through Tor (like we are in this project) will NOT make you anonymous on the web. There\u0026rsquo;s so much more to it than that.\nIn order to be anonymous in the real world, you might first try putting on a face covering. If you\u0026rsquo;re the only person wearing a face covering in public, you\u0026rsquo;re not really anonymous - you\u0026rsquo;re actually unique, because you\u0026rsquo;re sticking out like a sore thumb. Just because your face isn\u0026rsquo;t visible, it doesn\u0026rsquo;t make you anonymous.\nLet\u0026rsquo;s take the analogy a bit further. Suppose you are wearing a face covering, and everyone else around you is also wearing a face covering. Suddenly, your anonymity has increased.\nUsing this Tor proxy will make me anonymous now! Mission accomplished. Right? Not exactly. When our requests are proxied through Tor in this setup, we aren\u0026rsquo;t really taking proper steps to \u0026ldquo;blend in with the crowd\u0026rdquo; by any means.\nThe only reason Tor comes in handy in this architecture is to grant us a different IP address.\nEffectively, we could replace the Tor Docker service in this architecture with a VPN, and it would actually be better. However, for the sake of expediency, and for the sake of advocating the use of Tor, I decided to shoehorn it into this project. You should not use Tor if you don\u0026rsquo;t have to. People in the real world depend on Tor for their livelihood, and other people dedicate their time to offering their devices to others out of the goodness of their hearts.\nSo how do I do anything on the internet without being tracked online? First off, let\u0026rsquo;s agree on something: Just because you might \u0026ldquo;have nothing to hide\u0026rdquo;, doesn\u0026rsquo;t mean that you \u0026ldquo;should expose everything you do online to everyone\u0026rdquo;. That\u0026rsquo;s a big discussion in and of itself for another time.\nIn order to actually be anonymous online and browse the web (via a web browser), and following our analogy from earlier, everyone has to:\n Hide their face with a good face covering. Have the same physique. Wear the same clothing. Walk the same way. Talk to no one on the way to the destination. Make no indication of where you\u0026rsquo;re going to anyone watching until you\u0026rsquo;re at your destination.  All of this is done for you with the Tor browser.\nThe Tor browser should not be reconfigured at all from its defaults. By assimilating yourself into the masses, you have the ability to go where you want to go without the feeling of being watched, tracked, or potentially regretting clicking on an ad or page you didn\u0026rsquo;t mean to click. You can explore that rabbit hole without fear of becoming targeted by ads.\nI won\u0026rsquo;t be writing an in-depth guide on Tor here, but there are lots of resources online that can help you learn.\nWhat are the different files/changes in this repository, compared to the original? Added: Dockerfile The Dockerfile contains a standard automation for (somewhat) efficiently building the Python-based Docker image for this project. I definitely recommend you read the comments in the Dockerfile - I tried to annotate things to be as helpful as possible.\nAdded: docker-compose.yml The docker-compose.yml file is very important - in it, it contains two services, which are just containers that are configured to work together:\n the tor-privoxy service, discussed above the covid-vaccine-bot service itself  Take a look at this file and read each of my comments. Hopefully you\u0026rsquo;ll learn something new. Note: this file might be out of sync with master, make sure to check docker-compose.yml for differences before using this somewhere else! ðŸ™‚\nversion: \u0026#39;3.7\u0026#39; services: tor-privoxy: container_name: tor-privoxy image: dockage/tor-privoxy:latest ports: # by using \u0026#34;127.0.0.1:\u0026lt;external_port\u0026gt;:\u0026lt;internal_port\u0026gt;\u0026#34;, we restrict access to this Docker container to this system only - \u0026#34;127.0.0.1:9060:9050\u0026#34; # Tor proxy - \u0026#34;127.0.0.1:9061:9051\u0026#34; # Tor control port (not needed for this project) - \u0026#34;127.0.0.1:8118:8118\u0026#34; # Privoxy - a proxy with built-in ad-blocking lists restart: unless-stopped logging: # keeps logging to a max of ~2mb driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;200k\u0026#34; max-file: \u0026#34;10\u0026#34; covid-vaccine-bot: container_name: covid-vaccine-bot # network_mode: service:tor-privoxy # this will not work! you have to use the proxy image: covid-vaccine-bot:latest environment: # reads from the .env file located in this directory - HTTP_PROXY=http://tor-privoxy:8118  # the name of the tor-privoxy service resolves, thanks to docker\u0026#39;s DNS - HTTPS_PROXY=http://tor-privoxy:8118  # the name of the tor-privoxy service resolves, thanks to docker\u0026#39;s DNS - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}  # the .env file is automatically read by docker compose - TWILIO_ACCOUNT_SID=${TWILIO_ACCOUNT_SID}  # the .env file is automatically read by docker compose - TWILIO_AUTH_TOKEN=${TWILIO_AUTH_TOKEN}  # the .env file is automatically read by docker compose - TWILIO_TARGET_NUMBER=${TWILIO_TARGET_NUMBER}  # the .env file is automatically read by docker compose - TWILIO_SOURCE_NUMBER=${TWILIO_SOURCE_NUMBER}  # the .env file is automatically read by docker compose volumes: - ./covid-vaccine-bot.py:/src/covid-vaccine-bot.py  # allows you to restart the container instead of rebuilding the image when changing the source code tty: true # if tty is not set to true, python logs will not show up in real time depends_on: - tor-privoxy  # wait for the tor-privoxy service to start restart: unless-stopped logging: # keeps logging to a max of ~2mb driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;200k\u0026#34; max-file: \u0026#34;10\u0026#34; Added: .dockerignore and .gitignore The .dockerignore file is similar to .gitignore, if you\u0026rsquo;re familiar with it:\n .dockerignore instructs the docker build command to ignore files that match the patterns specified in this file.  This is useful because sometimes your repository contains large files that your Docker image might not even need in the first place.   .gitignore instructs git to exclude specified files; in this case, we ignore the .env file because it contains secret Slack webhooks and account credentials for Twilio.  Modified: covid-vaccine-bot.py I added some simple logic to handle environment variables in covid-vaccine-bot.py, which are defined in the .env file and automatically propagated into the container at runtime via Docker compose. Additionally, my Python auto-formatter did some code reformatting.\nSpecifically, os.environ.get('ENV_VARIABLE', '') is good practice, because if you just try to access a Python dictionary by a key that doesn\u0026rsquo;t exist, it throws an exception:\nos.environ[\u0026#39;NON_EXISTENT_KEY\u0026#39;] # throws exception, halts your code os.environ.get(\u0026#39;NON_EXISTENT_KEY\u0026#39;, \u0026#39;\u0026#39;) # simply returns \u0026#39;\u0026#39;, much better Added: Makefile The Makefile is pretty simple. If you\u0026rsquo;re not familiar with them, Makefiles allow developers to put frequently used commands/automated tasks into a single place. One of the best features about it is that you can use tab auto-completion out of the box by just simply defining a section, such as:\nbuild-image: docker build -t covid-vaccine-bot:latest . And you can execute this command by running:\nmake build-image # try tab completing as you type Final notes I\u0026rsquo;ll reiterate: Tynick did a fantastic job with this project - he was able to automate a tool to get multiple people a COVID vaccine! We both had a great time expanding and bringing this tool to our friends, and we feel honored to have helped get people vaccinated AND volunteering.\nI hope you\u0026rsquo;ve found this writeup informative. If you have any questions, visit charlesmknox.com and use any of the social contact URLs on the landing page. You can also visit charlesmknox.com/about to find ways to support me.\n","permalink":"https://charlesmknox.com/tech/software/general/covid-vaccine-bot/","summary":"ðŸ’‰ Introduction A friend of mine, Tynick, and I crossed paths on a project he started - he wanted to get a COVID vaccine by doing a volunteer shift. The two websites that offer volunteering signups occasionally had openings, but they were always taken almost immediately.\nSo, he had the brilliant idea to use some clever Python to automatically check the website, and notify him with Slack or Twilio whenever there\u0026rsquo;s a signup available.","title":"How we used automation to get COVID vaccines early"},{"content":" Check out the extension View the extension here! You can also view the source code here on GitHub.\nThe rest of this article is a writeup on the extension itself - enjoy!\nPreface Before talking about my addon directly, first I wanted to explain the context behind why it was created in the first place.\nAs a privacy enthusiast, I often find myself configuring my browser to match my needs. With Firefox, the multi-account containers extension provides a unique feature that separates Firefox from the rest of the browsers - the ability to separate cookies and browsing contexts on a per-tab basis.\n\nImage source: Mozilla addon store The multi-account containers addon is amazing! It allows you to willfully separate the different facets of your identity:\n A financial container for all of your finances An amazon container for all things related to Amazon only A facebook container for all things related to Facebook only (although, there is an official extension from Mozilla for this exact purpose) A work container for all things related to work only A reddit container for all things Reddit A hackernews container for all things HackerNews  etc. - you hopefully can see the pattern now.\nGaps Unfortunately, one of the gaps of the multi-account containers extension is that its interface was intended for people with only a few containers - if you\u0026rsquo;re like me and you have well over 100 containers, the UI becomes very cumbersome, because it lacks features like:\n filtering for a container modifying multiple containers at once, according to filter criteria deleting multiple containers at once opening multiple containers at once, with configurable URLs  So, I went ahead and wrote an extension that could do all of these things, with a few extra added features. Read on!\nThe rest of this blog post is a few select snippets of the actual GitHub README.md for the addon, which has some extra bits of info not covered here.\nQuick note The extension is certainly not dead, but I have not updated the extension since November 2020, primarily because:\n For all intents and purposes, it has stabilized It has all the features I need so far It has a small userbase, and no one has asked for more features (yet!) The features that I currently want to add to it are smaller than other projects I would rather work on  With all that being said - please, voice your opinions, open an issue/PR, or leave me a tip and a note about what you want out of the addon! I\u0026rsquo;m happy to pursue continued development work if people want me to.\nFeatures  Container search capability - filters your containers as you type.  Press enter to open the first result in the list (or the only result, if just one result remains). Simultaneously press ctrl and either click or enter to open the result(s) in a pinned state. Combine above shortcuts with shift to open all filtered containers at once.   Set a Default URL for containers - Any time you use this extension to open a tab, you can configure the tab to open a specific URL by default.  The URL settings are stored as part of the extension itself, and are independent of the Multi-Account Containers addon. It will not affect any existing settings, and will not change the behavior of which URLs are opened in which containers by default. Similar to above, press shift to bulk-set-default URLs for the current query.   Sticky Popup - If you want, you can check this box to keep the extension open while you click on different results (to open many containers) for your search. This mode feels very powerful to use. Set Name/Icon/Color mode - Allows you to quickly set one or more containers' icon, color, or name quickly. Find and Replace mode - Allows you to perform a find and replace on container names or default URLs. Duplication mode - Allows you to duplicate one or more containers returned by a search query.  Note: Duplication mode currently does not copy the default open-in-URL capability for multi-account containers, but it does duplicate default URLs defined for containers within this extension. (This extension currently does not have the capability to access information about default open-in-URLs for containers, which is stored in the extension settings for the multi-account containers extension storage in your browser\u0026rsquo;s settings)   Deletion mode - When checked, you can click on a container to delete it. This method of deletion is a bit quicker than the multi-account containers extension. You will be prompted for deletion more than once.  Caution: This can delete all of your containers if you\u0026rsquo;re not careful. Similar to above, press shift to bulk-delete containers returned by a query.   Keyboard shortcut to open the popup window is alt+shift+D. It will immediately focus the search box, so you can quickly filter for a container, press enter, and go.  Examples and Screenshots This section contains some recordings and walkthroughs of use cases for this extension. Hopefully, it helps clarify ways to leverage this extension as best as possible for readers.\nIf any of this is confusing, remember the basics:\n Press shift and click/enter to act on ALL results (bulk open tab/delete container/set URL action) Press ctrl and click/enter a result to open as pinned tab(s)  v0.0.10 Examples In v0.0.10, the features introduced were:\n Name Replace mode - Replaces a string in every matched container name URL Replace mode - Replaces a string in every matched container URL Set Color mode - Updates the color of all matched containers Set Icon mode - Updates the icon of all matched containers  See CHANGELOG.md for more changes.\nv0.0.10 Live Example In this example of the v0.0.10 release, the following actions are taken on all of the containers:\n Open as Tab(s) mode is used to open all of the containers shown. Set Default URL mode is used to set the URL for all containers shown. Set Name mode is used to set the name for all containers shown. Set Color mode is used to set the container\u0026rsquo;s icon\u0026rsquo;s color for all containers shown. Set Icon mode is used to set the container\u0026rsquo;s icon for all containers shown. Replace in Name mode is used to replace a string found in all containers' name. Replace in URL mode is used to replace a string found in the containers' URL. Duplicate mode is used to duplicate all containers shown. Delete mode is used to delete all containers shown.  v0.0.10 Screenshots Here\u0026rsquo;s how the extension looks when you click on it:\nAll available modes in v0.0.10:\nSetting default container URLs by pressing the shift key and enter or clicking a container result, in v0.0.10:\nDefault URLs applied in v0.0.10:\nChanging the name of multiple containers at once in v0.0.10:\nUsing find and replace in container URLs in v0.0.10 - this screen is preceded by two prompts (one for the \u0026ldquo;find\u0026rdquo; string, the other for the \u0026ldquo;replace\u0026rdquo; string):\nTips Make sure to read all of the features before perusing the tips to get the most out of the extension.\nContainer naming convention suggestions It may be worth considering using certain naming conventions for your containers to help perform bulk actions, such as:\nfinance-bankA finance-bankB *email-gmail *email-protonmail email-tutanota dev-github-personal dev-github-work dev-gitlab-personal dev-gitlab-work social-reddit-personal social-reddit-public social-reddit-work *chat-discord-personal chat-discord-work *chat-slack-work chat-slack-personal media-streaming-netflix media-streaming-plex media-streaming-hulu *media-streaming-spotify google-personal google-work duckduckgo-ddg The containers starting with * could be considered as permanently pinned tabs, so you can do a quick search for * and press ctrl+shift+enter to get the results. Note that sometimes URL\u0026rsquo;s can have a * character, so you may want to experiment with what character works best for quickly filtering your preferred pinned tabs.\nFAQ  When duplicating an existing container, does it also duplicate cookies and other session information?  No, it creates a fresh container with only the same basic metadata as the original container, such as color/name/icon.    Warnings This container management extension is dangerously powerful. If you\u0026rsquo;re not careful, you can delete all of your containers by turning on \u0026ldquo;Delete Mode\u0026rdquo;, pressing shift+enter, and pressing \u0026ldquo;OK\u0026rdquo; to the prompts. At this time, the extension doesn\u0026rsquo;t support undoing operations or rolling back commands. You\u0026rsquo;ve been warned!\nFuture Features Open to suggestions.\n Ctrl+Click to select individual results instead of it just opening pinned tabs as it does now. Ideal behavior would be something like using ctrl to select individual results from the list, and then using a checkbox to open as pinned (or something similar to that). Sorting - Sort results according to criteria Container Import/Export - This will be a bit tricky though, since this extension doesn\u0026rsquo;t have control over url-to-container associations made in the multi-account containers extension. Saved searches - Saving the results and possibly binding to a keystroke might be useful. Favorite/Tagged containers - Adding a \u0026ldquo;star\u0026rdquo; capability to certain tabs so that you can filter them easier. For now, a workaround is using the naming conventions suggested in the tips section. Bulk regular expression actions - Actions on containers from search results according to regular expressions might be useful. Metrics - Track simple interaction data (locally only, privacy is important) so you can look back on your interactions with containers. Accessibility Need to conform to accessibility standards.  Community If you have suggestions, please feel free to voice them on GitHub. Thank you for using my extension and reading this far!\nAttributions The addon comes packaged with Bootstrap, and includes a distribution of it in its source code. See the license here.\n  ","permalink":"https://charlesmknox.com/portfolio/webextensions/ffcontainershelper/","summary":"Check out the extension View the extension here! You can also view the source code here on GitHub.\nThe rest of this article is a writeup on the extension itself - enjoy!\nPreface Before talking about my addon directly, first I wanted to explain the context behind why it was created in the first place.\nAs a privacy enthusiast, I often find myself configuring my browser to match my needs.","title":"Firefox Browser Addon Web Extension: Containers Helper"},{"content":"My name is Charles Knox. I am a professional software developer, and have nearly a decade of industry software development and information technology experience.\nI write software that I hope people will use. I am always working on solutions that will make your life easier.\nI enjoy writing about techie-focused topics, as well as things I learn as I pursue of a life of healthy living.\nOccasionally, I\u0026rsquo;ll try and post some privacy-respecting ads or affiliate links for products or groups that I care about. I will never plug a company or corporate entity on this site that isn\u0026rsquo;t worthy.\nIf you see an advertisement that looks suspicious, please email me and I\u0026rsquo;ll investigate.\nBlog This blog\u0026rsquo;s source code is publicly viewable and contained here on GitHub.\nContact ðŸ“§ Contact me via email, this is the preferred method. I use a mail forwarding service for all of my public-facing email links - when you send this to me, it will go directly to my inbox.\nOther social platforms For social outlets that are not on the front page:\n https://gitlab.com/charles-m-knox https://stackoverflow.com/users/3798673/charles-m-knox https://dev.to/charlesmknox https://medium.com/@charles-m-knox https://opencollective.com/charles-m-knox  Discord server I also have recently created a Discord server where you can connect with me. You can join here:\nhttps://discord.gg/94sX5ex3Ht\nPrivacy and data I do not personally collect any data about you when you visit my site.\nHowever, there are a few entities that may track you in different ways when you visit the site:\n Cloudflare - All requests go through Cloudflare first; it proxies all connections.  I can see basic geographical information about everyone who visits my site through Cloudflare. I cannot (currently to my knowledge) see what pages were visited via Cloudflare.   GitHub - This site is hosted using GitHub pages.  Microsoft owns GitHub, so if you have any objections to Microsoft seeing that you\u0026rsquo;re visiting this site, I understand, but ultimately I have decided that hosting with GitHub pages is acceptable for the vast majority of personal privacy/security threat models. You\u0026rsquo;ll probably be fine. Because all connections are proxied through Cloudflare, Microsoft/GitHub receive information about Cloudflare, but it is likely that Cloudflare may also provide X-Forwarded-For headers that indicate the originating IP address to GitHub, as this is fairly common.   Any ad networks that run through this site will be running JavaScript that I do not have control over.  I will list out every ad network that runs on this site. I hope you\u0026rsquo;ll trust my choice in ad networks; I scrutinize them fairly carefully before deciding to go forward with one. While it is very important to me that you see the ads on this site, I understand if you have a serious privacy concern and need to avoid being tracked on the web. I rely on income for this site, so please consider disabling it if an advertising network that I\u0026rsquo;ve vetted does not violate your privacy threat model. You should assume that arbitrary ad-driven JavaScript can uniquely fingerprint you and feed that data into anywhere; I have not inspected the source code for the ads myself, and even though the ad networks I choose assure me they do not track users, you cannot be 100% certain.   Any third-party CDN\u0026rsquo;s that deliver JavaScript, CSS, or other assets needed in order for your browser to run this site.  I don\u0026rsquo;t believe my site currently leverages any third-party CDN\u0026rsquo;s, but if that changes, it will likely be a standard CDN such as Amazon CloudFront, JSDelivr, or some other common CDN.    Ads, referrals, affiliate links As of writing this article on 2/28/2021, I am not currently promoting anything by means of affiliate programs. This is likely to change, but in general, my personal affiliate policy is as follows:\n I will generally not push any products I do not believe in. If I do, I will write a clear disclaimer. I generally try to trust a company before I would even purchase something from there myself.  I generally will try to apply the same scrutiny to products I recommend, with disclaimers noting any objections I have.    You should assume that any link to any product on my site is an affiliate link. I\u0026rsquo;ll try and add disclaimers to highlight them where possible.\nList of current ad networks  ContextCue - ContextCue is a privacy-focused advertising network that allows advertisers and publishers to grow their business with fast, lightweight, contextual ads that don\u0026rsquo;t track, collect, or store user data  This ad network seems like it was written by regular software developers with good intentions, and written for the modern world. Consider removing ContextCue from your ad blocker globally. ContextCue allows me to pick a variety of ad topics. I will pick ads topics according to the article I\u0026rsquo;m writing, so you may see e.g. finance or computer related topics. This is not you being tracked, this is me curating advertisements according to what I\u0026rsquo;m writing.    Former Ad Networks If you are viewing cached or old versions of webpages, you may see some of these embeds, which I no longer use.\n AdEx - A display ad network for reduced ad fraud and increased user privacy.  This site uses the DAI stablecoin cryptocurrency for payment processing. As a result, you may see crypto-related advertising from time to time. Stay vigilant, and use best judgement when managing cryptocurrencies, as mistakes can get your funds taken away.    Ways to support me directly If you have an ad blocker turned on, and absolutely don\u0026rsquo;t want to turn it off, I understand. I don\u0026rsquo;t make much money from the ads anyways, since I personally block them and I don\u0026rsquo;t really like them.\nHere are some ways to send me a tip:\nGitHub Sponsors  Buy me a coffee! ðŸ¥‘  Buy me a Coffee healthy avocado! ðŸ¥‘  I can\u0026rsquo;t drink much coffee, but I do need to eat healthy, and avocados are expensive! ðŸ™‚  OpenCollective You can see my OpenCollective profile here. As of writing this (3/15/2021), I do not have any repositories with \u0026gt;100 GitHub stars, so I cannot host any repositories there. As soon as I do, I\u0026rsquo;ll add them to OpenCollective, and people can make contributions.\nTip via crypto currencies  Bitcoin: 3NczPNsQ5FGJ27qZwiAkBZfWYfPV3uAMoK Bitcoin Cash: 1DU7e1TNumajgaZLTk8LYADBZUidKvpGKH Basic Attention Token (BAT): 0xa672b3C63Be650c29AbaD738d1CB1B83e817c43e DAI: 0xB780bf6A9aB821D1e2D400835154e7d3dD5409c6 Stellar Lumens: GDQP2KPQGKIHYJGXNUIYOMHARUARCA7DJT5FO2FFOOKY3B2WSQHG4W37:::ucl:::1373794829  Other cryptos: Contact me and ask, I\u0026rsquo;m happy to oblige.\nBrave I participate in the Brave Rewards for Creators program. If you\u0026rsquo;re using the Brave browser, you should see my site listed as verified.\n","permalink":"https://charlesmknox.com/about/","summary":"My name is Charles Knox. I am a professional software developer, and have nearly a decade of industry software development and information technology experience.\nI write software that I hope people will use. I am always working on solutions that will make your life easier.\nI enjoy writing about techie-focused topics, as well as things I learn as I pursue of a life of healthy living.\nOccasionally, I\u0026rsquo;ll try and post some privacy-respecting ads or affiliate links for products or groups that I care about.","title":"About"},{"content":"If you want to get on this page, visit the About page \u0026ldquo;Ways to support me\u0026rdquo; section!\nSupporters Bob Haines Bob Haines supported my work on the Firefox Containers Helper extension. I am honored and humbled that he took the time to make a contribution and a feature request, and as my first supporter, he will forever have my gratitude. Thank you Bob!\nBob contributed to my healthy eating and coding habits via Buy me a Coffee: https://www.buymeacoffee.com/charles.m.knox\n Thank you for the firefox-containers-helper. Your module is helping me register older senior citizens for the COVID-19 vaccine. Would love it if you could add click then shift-click selection to the addon. Thanks again for the excellent module!\n    Bob also left an awesome review on the Firefox addons store:\n Just discovered this excellent extension. Super helpful for managing large numbers of containers. I\u0026rsquo;ve found it extremely useful to help me book vaccine appointments for senior citizens. Thanks for this super useful module!\n    I checked with Bob to see if it was OK for me to share his contact information - if you want to reach out to him and say thank you for supporting the community, feel free!\n Bob on LinkedIn @rjhaines on Twitter  ","permalink":"https://charlesmknox.com/supporters/","summary":"If you want to get on this page, visit the About page \u0026ldquo;Ways to support me\u0026rdquo; section!\nSupporters Bob Haines Bob Haines supported my work on the Firefox Containers Helper extension. I am honored and humbled that he took the time to make a contribution and a feature request, and as my first supporter, he will forever have my gratitude. Thank you Bob!\nBob contributed to my healthy eating and coding habits via Buy me a Coffee: https://www.","title":"Supporters"},{"content":" Before switching to Hugo, I wrote my own full-stack website, and hosted it on AWS. The site was written in:\n Frontend: Angular 9 Middleware/web server: Go 1.13 Data: Static markdown files  Screenshots Because the website is not statically generated and requires a live API, and I do not want to continue to provide uptime for the API, I\u0026rsquo;ll be providing a few screen captures of the site in action.\nFor some of the below screenshots, click on it to view a larger version. Images are compressed significantly in order to avoid any strain on my hosting providers. I would have exported them statically and render them here, but it\u0026rsquo;s a messy process and will bloat this site\u0026rsquo;s footprint more due to the fact that each static export would have included all of the CSS, webfonts, and images for each screenshot.\nAttributions \nArticles Listing Click the image for a larger version that isn\u0026rsquo;t cut off.\n\nAn Article Click the image for a larger version (of a different article, on sleeping) that isn\u0026rsquo;t cut off. This one is quite long.\n\n  Technical Discussion The site\u0026rsquo;s API layer was written in Go 1.13, was served via Docker and used a no-DB architecture that focused purely on storing files in a ./src directory:\nIt\u0026rsquo;s pretty clear that the articles have a specific naming convention. In order to quickly generate an article, I created a bash script called ./gen_article.sh:\n#!/bin/bash -e  pubdate=$(./gen_isodate.js) read -p \u0026#34;Publication title: \u0026#34; pubtitle read -p \u0026#34;Publication category: \u0026#34; pubcategory echo \u0026#34;# ${pubtitle}\u0026#34; \u0026gt; \u0026#34;./src/${pubdate}___${pubcategory}___${pubtitle}.md\u0026#34; echo \u0026#34;Done!\u0026#34; Note that ./gen_isodate.js does nothing more than this:\n#!/usr/bin/env node console.log(new Date().toISOString()); I probably could have just replaced it with the following Bash command though, instead of bringing Node into the mix - but after a certain point it was just easier to leave it, since Node was already a dependency in this project anyways:\ndate -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;  Side note: Normally I\u0026rsquo;m very strict about what dependencies I use. I always opt for the bare minimum of what\u0026rsquo;s needed in order to get the job done. This was honestly just me being lazy, which was OK, because it was only me working on this project ðŸ˜„\n Efficiency/Optimizations I didn\u0026rsquo;t specifically focus on optimizing the frontend, but I did get to leverage industry-standard optimizations in the middleware layer.\nThe footprint of the site\u0026rsquo;s API is as follows:\n Docker image size: 13.4 MB CPU Usage: \u0026lt;1% normally, with un-noticeable CPU consumption when reading from disk (no time to run more detailed benchmarks; not really worth it) Runtime memory usage: 6.5MiB, serving 14 documents from memory  The footprint of the frontend is as follows:\n Build time: On a 10th-gen Intel i7, build time (via ng build --prod) is 26306ms Site size: 1.5MB, including all CSS; excludes web fonts and images loaded externally  API Dockerfile The following builder pattern works very well for creating a minimal image and binary (13.4MB mentioned above).\nFROMgolang:alpine AS builderWORKDIR/go/src/currentCOPY go.mod /go/src/currentRUN go mod downloadCOPY . /go/src/currentWORKDIR/go/src/currentRUN go build -vFROMalpine:latestCOPY --from=builder /go/src/current/content /contentCMD [\u0026#34;/content\u0026#34;]Source Code I don\u0026rsquo;t feel it\u0026rsquo;s appropriate to release the source code to the public on this project. I\u0026rsquo;m definitely a huge open source software enthusiast and advocate, but in this case, my reasons for not wanting to publish are:\n It\u0026rsquo;s very unpolished; there\u0026rsquo;s a lot of \u0026ldquo;tinker\u0026rdquo; code that I ended up just commenting out. It\u0026rsquo;s not worth the time to clean it up. All of the code/functionality in the API that actually matters was fed into the Light Sites project, mentioned below. Due to how fast Go and Angular iterate their frameworks, I felt that releasing older, unpolished, random code to the the open source community might do more harm than good - I imagine someone stumbling upon this codebase and naively viewing some of it as \u0026ldquo;best practice\u0026rdquo;.  As with any software project, informal or not, careful decisions need to be made when it comes to deciding when to release.\nHow the API reads \u0026amp; serves data When designing the API, I wanted to take an approach that might prevent excessive disk reads, in the event that someone nefarious ever tried spamming requests to my API. To handle this, I opted to read every .md file under ./src and load it into memory every 30 minutes. Then, when someone makes a request to my API for the content they asked for, I\u0026rsquo;d be serving it from RAM instead of doing any disk IO.\nIf I\u0026rsquo;ve got 14 files and I read from disk every 30 minutes, you could say that it\u0026rsquo;s equivalent to someone hitting the page approximately every 2 minutes, in terms of the number of files read from disk. My blog was never popular enough to justify that, but it seemed like a solid approach to getting the most out of that already-minimalist RAM footprint of about 6.5MiB (mentioned in the efficiency/optimizations section above).\nIn essence, this was a pure-RAM-cache API that served its purpose fairly well. There\u0026rsquo;s always room for optimizations and improvements in any software that we write, but this was \u0026ldquo;good enough\u0026rdquo;.\nFor larger-scale websites, the API footprint would have grown, and the average number of disk-reads per minute would go up, but all things considered it\u0026rsquo;s still a reasonable approach.\nA better approach would be not to use intervals at all. Instead, do a \u0026ldquo;read-once\u0026rdquo; whenever an API request comes in for any article that hasn\u0026rsquo;t been retrieved in the last e.g. 2 hours, then cache the contents of that page in memory. Many caching layers in production-grade software do some variant of this.\nDownsides The biggest downside with writing a website with an always-on API is ensuring uptime. Even with Docker\u0026rsquo;s --restart unless-stopped option, managing an AWS VM just to keep the API alive isn\u0026rsquo;t worth it. For this kind of website, statically generated files are by far better, because I can just generate the files and upload the site to GitHub Pages, AWS S3, etc. and spend zero energy and time managing it beyond that. Reusing other services to save yourself time is almost always the way to go.\nWhere the results of this project went After writing this, I took a lot of the learnings from statically parsing Markdown files, traversing HTML node trees, and API layers to write Light Sites. This was another fun project that was a successor to this API; it was unit tested more comprehensively (\u0026gt;95% coverage), has more customizability, and had more focused time investment. Since Light Sites was primarily a portfolio builder project, I plan to do a writeup on it here as well.\nAdditionally, some of the techniques I used and implemented in the Angular frontend have stuck with me for years and have been very useful. Personal projects always yield valuable learning.\nThanks for reading!\n  ","permalink":"https://charlesmknox.com/portfolio/fullstack/blog-site/","summary":"Before switching to Hugo, I wrote my own full-stack website, and hosted it on AWS. The site was written in:\n Frontend: Angular 9 Middleware/web server: Go 1.13 Data: Static markdown files  Screenshots Because the website is not statically generated and requires a live API, and I do not want to continue to provide uptime for the API, I\u0026rsquo;ll be providing a few screen captures of the site in action.","title":"Professional Blog Site"},{"content":" Go 1.16 was released - here are the release notes.\nThe thing that caught my eye was that io/ioutil has been deprecated - here\u0026rsquo;s what you need to know\n The io/ioutil package has turned out to be a poorly defined and hard to understand collection of things. All functionality provided by the package has been moved to other packages. The io/ioutil package remains and will continue to work as before, but we encourage new code to use the new definitions in the io and os packages. Here is a list of the new locations of the names exported by io/ioutil:\n  Discard =\u0026gt; io.Discard NopCloser =\u0026gt; io.NopCloser ReadAll =\u0026gt; io.ReadAll ReadDir =\u0026gt; os.ReadDir (note: returns a slice of os.DirEntry rather than a slice of fs.FileInfo) ReadFile =\u0026gt; os.ReadFile TempDir =\u0026gt; os.MkdirTemp TempFile =\u0026gt; os.CreateTemp WriteFile =\u0026gt; os.WriteFile    ","permalink":"https://charlesmknox.com/tech/software/go/go-1.16-deprecating-ioutil/","summary":"Go 1.16 was released - here are the release notes.\nThe thing that caught my eye was that io/ioutil has been deprecated - here\u0026rsquo;s what you need to know\n The io/ioutil package has turned out to be a poorly defined and hard to understand collection of things. All functionality provided by the package has been moved to other packages. The io/ioutil package remains and will continue to work as before, but we encourage new code to use the new definitions in the io and os packages.","title":"Go 1.16 Deprecating ioutil"},{"content":" I\u0026rsquo;ve been taking the time regularly in my life to prioritize learning. I\u0026rsquo;ve been on the lookout for new things to learn.\nSo, I explored options for online learning. I found Coursera. A few of the top recommended online classes included a particularly interesting class called Learning How to Learn.\nIn this class, you really do learn how to properly learn and engage with new and confusing content. Let\u0026rsquo;s take a look at some of the most profound things I learned from the class.\nBrain Modes The first thing that caught my attention was the concept diffuse and focused brain modes.\n\nThink about what it\u0026rsquo;s like when you try to understand a difficult and novel concept, such as an abstract math or physics equation. When taking a fresh, new look at hard material like this, I tend to read back and forth, hoping to latch on to any piece of the material, and form associations with other pieces of the material that I\u0026rsquo;m trying to learn. During this process, I\u0026rsquo;m confused, uncertain of the topic, and generally not having a good time understanding what\u0026rsquo;s going on.\nIt turns out that this is actually a typical brain mode, called the diffuse mode. Existing in the diffuse mode means that you\u0026rsquo;re trying desperately to grab on to any novel concept you can and form new neural connections. This is where creative ideas and abstract concepts come to fruition. You want to remain in the diffuse mode so that you can encourage the inception of novel thoughts.\nThe downside about the diffuse brain mode is that it\u0026rsquo;s hard to remain in this mode for long - it\u0026rsquo;s frustrating, confusing, and downright unsustainable to effectively always learn in this way. We all eventually reach a point where we just can\u0026rsquo;t learn anymore, and the mind\u0026rsquo;s ability to learn has reached a saturation point that will no longer allow more novel thoughts to form.\nThis is where the focused mode comes into play. When you\u0026rsquo;re performing a routine task, or doing something easy and sensible, you\u0026rsquo;re switched into the focused mode. In this mode, I personally choose to lift some weights, eat a few bites, have some quick chats, etc. This is a procedural, straightforward mode that is meant to sharply contrast the more difficult and abstract diffuse mode.\nSalvador Dali Salvador Dali and Thomas Edison used a trick to enable diffuse mode thoughts on demand. They would hold something in their hand that makes a loud noise when dropped, such as keys or ball bearings, and hold them in their hands while drifting off into a brief nap. As soon as the nap takes over and muscle control is lost, the item in their hands would drop to the floor, and they would snap out of it.\nThe thoughts that were floating around in their diffuse brain are likely novel ideas and thoughts that they wouldn\u0026rsquo;t have come to otherwise.\nThe Technique When you switch between focused and diffuse modes intentionally, your brain will actually form new neural pathways more efficiently.\nStay diffuse for a while, then take a quick break in focused mode. There is a technique that implements this: The Pomodoro technique.\nThe concept is simple: Learn for 25 minutes, take a break for 5 minutes.\nDuring your learning time, don\u0026rsquo;t break your focus. Just commit to learning for 25 minutes.\nThen, when break time comes, do something easy. Perhaps you could frame it as a reward for all of your hard work. As I said earlier, I personally use the time to lift some weights, eat some bites of food, use the restroom, grab water, say hello to my significant other, etc.\nPomodoro Software I use Ubuntu 20.04 as my main system, so I\u0026rsquo;ll recommend the Gnome Pomodoro software project.\n\nInstall it using its recommended installation method, and use the gnome-tweak-tool or the official Extensions application (built in to newer Gnome desktop versions) to ensure it\u0026rsquo;s enabled. You can start it as a standalone application as well.\n  Process vs Product By focusing on process instead of product, it allows you to naturally work towards a goal instead of getting stressed about the end result. For example, following a process of spending 25 minutes on a task is more likely to succeed than trying to get the whole task done within 25 minutes. This principle operates complementary to the Pomodoro technique.\nSleep Scientific observations presented in the course confirm that there are actually small \u0026ldquo;toxic\u0026rdquo; substances that build up in the brain throughout the day. When the mind is at rest, natural processes wash away the undesirable substances. A healthy amount of sleep is necessary to refresh the mind.\nI wish I could be more scientifically specific than this, but the class did not offer more insights - there are references presented that detailed more info, but I do not have the time to go into them in more detail.\nWorking Memory and Long Term Memory When we are focusing our minds on a task, we generally only are able to keep 4 things in what\u0026rsquo;s called \u0026ldquo;working memory\u0026rdquo;. For example, when doing a math problem, you may only be able to keep track of a few variables and their values while working through the problem at a time.\nLong term memory, on the other hand, can complement your working memory by helping you recall concepts related to an item in working memory.\nTo keep working memory and long term memory at their best, use techniques like deliberate practice.\nDeliberate Practice Spreading out your learning by deliberately practicing a few problems regularly (for example once a day, or every couple days) helps facilitate absorption of the content and better understanding. This is one of the more important concepts to bake into your learning routine.\nChunking Chunks are pieces of information that are bound together through use and meaning. They\u0026rsquo;re built with focused, undivided attention and understanding of the basic idea at hand. Chunks are critically important to learning because they allow you to form a solid block of related information that you can access quickly. The brain is quickly capable of recalling and leveraging a well-formed chunk.\nThe true advantage of using chunks comes to fruition when they can be used with other information, through a process called transfer.\nTransfer Piggy-backing on the concept of chunking, transfer is the idea that a chunk can be interleaved with other areas of your mind (either via diffuse or focused modes of thinking!). This is really important because historically, clever people taking a chunk from one area of study in another field has lead to great innovations that might not otherwise have occurred.\n  Illusions of Competence When attempting to learn a new subject, it\u0026rsquo;s important to acknowledge that there are certain learning methods that we try that do not actually yield any results. Proven, effective methods need to be discussed.\nThe first of these methods is to test yourself on the topics you\u0026rsquo;re trying to learn. You can read all of the solutions you want, and re-read all of the text you want, but ultimately you\u0026rsquo;re taking an easier path than if you had just tested yourself.\nThe next is recall. Highlighting, underlining, and re-reading are not effective learning methods. Recall is very effective, and it\u0026rsquo;s easy. Simply look away from the text you just read, and try to recall it immediately from memory. If you fail, go over it again, and try it again until it sticks.\nMake mistakes when learning. Don\u0026rsquo;t be afraid to mess up. This is one of my personal appreciation points with a system like Coursera - you can frequently try again without penalty on most assignments, aside from, say, only being able to submit an assignment once every X hours.\nRecall the segment on deliberate practice - practice makes perfect, and this applies to your learning. Set up practice problems for yourself regularly.\nEinstellung is a German word for \u0026ldquo;mindset\u0026rdquo;. An initial thought, pattern, or idea that already exists in your mind may prevent you from learning new things. Simply being made aware of the fact that you may be succumbing to einstellung in your studies can help you learn more effectively.\nThe last note for this section is about The Law of Serendipity. Lady Luck favors the one who tries. In practice, what this means is that if you just try to learn, regardless of what result you think you\u0026rsquo;ll get, you\u0026rsquo;re already more likely to succeed than the version of you that didn\u0026rsquo;t try.\nProcrastination Using the techniques outlined in the Pomodoro technique, and the process vs product discussion, procrastination should become less of a beast. That being said, there are still a couple ways to beat it still.\nCommit yourself to certain routines and tasks every day. I personally do not struggle with this, but perhaps you do. A well-structure daily lifestyle will yield excellent benefits, because you are exerting less willpower by simply following a successful routine.\nRewards come after the task or process is finished, not before, and not during. Part of successful, good habit-forming technique involves setting yourself up with a reward for completing the task. Deliberately treat yourself to something that you look forward to when your task is done.\nWatch for procrastionation cues. Find a quiet place to work with minimal distractions. Silence your phone. Turn off ALL push notifications, everywhere (this is a major one that I recommend). You are in control of the flow of information and can always check your apps later, notifications must not control your attention.\nThere were a few other tips in the procrastination section of the class, that I did not touch upon here, because I did not personally benefit from them as much. I highly recommend taking the class to continue learning.\nMemory When trying to commit something to memory, consider using a goofy, fun, or vivid set of imagery. In the class, an interview with a competitive professional memory athlete takes place (this was new to me), and in the interview, the athlete is able to recite a long string of digits forwards and backwards - so long as the interviewer read them about 1 second apart.\nThe athlete describes his process as follows:\n Create a \u0026ldquo;memory palace\u0026rdquo; that your mind walks through When presented with each number in the sequence, recall a (pre-selected by you) familiar image that you\u0026rsquo;ve associated with the number, and navigate to it in your memory palace. The image could be a shovel, or a tree, or an animal.  So, to best leverage memory, associate it with vivid mental imagery - even take it a level further and combine it with helpful metaphors and analogies.\nMetaphors and Analogies This may not be as much of a surprise, but using metaphors and analogies are extremely helpful with the process of understanding.\n  Studying Studying with friends, colleagues, etc. The ability to discuss ideas and learn from others is essential. Study groups are important - make every effort to start one or get involved in one.\nKeep study groups focused. A good, healthy study group arrives on time for study sessions. All members of a good study group have prepared for the session by reading the material in advance, and even coming with questions ready for discussion. Talking about non-essential things will greatly diminish the value of your study groups.\nTry preparing for your learning in different environments. Studying in the same room forever creates environmental cues that only exist in that room. When you take a test, you\u0026rsquo;ll be in a different room that you\u0026rsquo;re unfamiliar with.\nTest Taking The class ends with this material, and I found it to be extremely useful. How do you get better at taking tests? Let\u0026rsquo;s dig in.\nUpon receiving the test questions (presumably in a typical timed testing environment), immediately start working on a hard problem. Make progress on the hard problem until you get stuck. Then, immediately pivot to the easier questions in the test. This allows your \u0026ldquo;second brain\u0026rdquo; (your diffuse mode) to work in the background while your focused mode handles the easy items. This is important!\nTest taking releases cortisol, a stress hormone that substantially impacts how you feel. Try to shift your mentality from something like, \u0026ldquo;this test has me feeling scared\u0026rdquo;, to something more like, \u0026ldquo;this test has me excited to do my best\u0026rdquo;, you\u0026rsquo;ll notice a difference in your test taking abilities.\nAs part of managing your response to the stress hormone cortisol, you should also try deep breathing from your stomach. Relax your stomach, place your hand on it, and draw a deep breath.\nFinally, make sure to get a lot of sleep each of the nights leading up to the exam.\nFinal Note If you haven\u0026rsquo;t taken the class yet, it\u0026rsquo;s free. Do it here: Learning How to Learn\nThanks,\nChuck\n  ","permalink":"https://charlesmknox.com/health/general/learning-how-to-learn/","summary":"I\u0026rsquo;ve been taking the time regularly in my life to prioritize learning. I\u0026rsquo;ve been on the lookout for new things to learn.\nSo, I explored options for online learning. I found Coursera. A few of the top recommended online classes included a particularly interesting class called Learning How to Learn.\nIn this class, you really do learn how to properly learn and engage with new and confusing content. Let\u0026rsquo;s take a look at some of the most profound things I learned from the class.","title":"Learning How to Learn"},{"content":" Recently I stumbled upon a Reddit post in the ChangeMyView subreddit that suggested that it should be \u0026ldquo;the standard\u0026rdquo; for employers to allow 20-minute naps around noon.\nAs someone that values having some downtime around lunch to close my eyes and nap, this topic was worth reviewing in detail. What are the benefits of taking naps? How long should naps actually be? When should they be taken? Can an employer allow an employee to take naps? Continue reading for answers.\nEssential Notes on Naps The following essential notes on naps are backed up by a variety of different (seemingly) reputable sources that I\u0026rsquo;ve researched.\n Keep naps to 20 minutes.  If you\u0026rsquo;re going to nap longer, make sure it\u0026rsquo;s 30 minutes max, or 90 minutes minimum. Napping for a range between the 30-90 minute suggestions will leave you feeling dazed because you\u0026rsquo;ve reached deeper stages of sleep and then subsequently interrupted them prematurely. If you\u0026rsquo;re sleep deprived (5 hours of sleep), the optimal amount of time to nap may start from around 10 minutes.   The time to nap after eating is an important factor.  After eating, the body eventually enters a state called reactive hypoglycemia, which is a drop in blood sugar levels that occurs a few hours after eating. Blood sugar levels increase when eating processed foods and sugars. Your body will feel desperately like it needs a nap when reaching this state, which typically is reached after about 2-4 hours after eating. Reactive hypoglycemia can be counteracted by spreading out snacks throughout the day, and reducing sugars and processed foods. Fruits contain sugars that are packaged with large amounts of fiber, so eating fruits is a great substitute for satisfying sweet cravings while reducing unhealthy sugar intake.   Don\u0026rsquo;t nap later in the afternoon.  Taking naps after 3pm can actually interfere with regular scheduled sleep at night. Later in this article, we present a table that shows when to properly take a nap based on when you wake up, need to be at work, need to go to bed, etc. This is useful for individuals with non-standard shifts.      Sleep Stages \u0026amp; Cycles Taking a brief, 20-minute nap allows the body to enter the first 1-3 stages of sleep, without entering SWS (Slow-wave sleep), which occurs a portion of the way through stage 3 and all of stage 4. This is an example of sleep stages during a 90-minute cycle:\n\nSleep cycles are a topic with going into at a later time, but the takeaway here is that sleep inertia builds after 20-50% through stage 3. The goal of a proper nap is to reduce the interruption of sleep inertia while still preserving the proper benefits.\nBenefits of a Midday Nap Naps have been linked with the possibility of:\n Increasing physical awareness  Gaining increased psychomotor speed Better reaction time to all external stimuli Generally increased sense of awareness and alertness   Enhanced learning  Improvements in focus and memory The ability to learn and retain new information is enhanced after ending a nap In infants, a notable increase in efficiency for learning new words   Biochemistry and physical improvements  Lowered blood pressure, with changes on par with cutting salt and alcohol consumption from a diet Counteract jet lag via \u0026ldquo;the NASA nap\u0026rdquo;.    The \u0026ldquo;NASA nap\u0026rdquo; article (mentioned in the last bullet point above) notes that naps of some longer lengths (such as 2, 3, or 4 hours) actually tend to blend together and deliver similar results - meaning that a 2-hour nap could be just as beneficial as a 4-hour nap.\nThe study also mentions that the \u0026ldquo;rest group\u0026rdquo; was assigned a 40-minute nap time, but the circumstances were engineered carefully to prevent subjects from entering SWS (Slow-wave sleep).\nDownsides of a Midday Nap Napping requires a bit of foundational discipline. Ensure that your naps follow the advice sprinkled throughout this article.\n Midday naps must never exceed 20 minutes  Or else exiting the nap will feel as if a deep sleep has been interrupted.   Risk of interrupting circadian rhythm  Sleeping at the wrong time of day or for too long can interrupt the circadian rhythm.   Habitually taking longer naps increases risk of cardiovascular disease and all-cause mortality  Specifically, napping for longer than an hour during the day is associated with generally being less healthy. If you\u0026rsquo;re finding yourself taking longer naps on a regular basis, evaluate your daily routine and seek ways to improve your routine. Use the table in this article below for an example of an optimal routine, and try to eat healthier and exercise for a few weeks until the rhythm changes.    How Can Employers and Employees Pragmatically Integrate Nap Time? Employers need to be able to understand when naps are allowed, and employees should understand how to position their daily routine to work within the allotted time for naps.\nEmployers: Mandatory Break Time can be Nap Time The US Department of Labor provides paid rest periods of short duration running from 5-20 minutes, and even notes that employee efficiency is improved when these periods are utilized.\nThe maximum allowable nap time under our guidelines is 20 minutes. Therefore, solely under these guidelines, a 20-minute nap is already employer-sponsored.\nHowever, the Department of Labor website does not appear to mention the maximum number of break periods allowed. For this, I\u0026rsquo;ll refer to California law, which states that a typical 8-hour shift entitles employees to 30 minutes of unpaid lunch and 20 minutes of paid rest time. If you intend to integrate napping into your break period, you may have to ensure that the 20-minute nap is within this legal boundary.\nEmployees: When to Nap According to Shift Start Time For an 8-hour workday, assuming a schedule of sleeping from 10pm-6am and working from 9am-5pm, and lunch at 12pm, the latest possible time for a nap might be around 7.5 hours before bedtime, which lands an ideal nap time at approximately 2:30pm. Ensure that your nap ends before 3pm for best results.\nThis table shows an ideal circadian schedule, including 8 hours of sleep, 16 hours awake, 8 hours work, and a proper midday nap:\n   Wake Up Shift Start Lunch Nap Shift End Bed     Start +3 hrs +6 hrs +8.5 hrs +11 hrs +16 hrs   12:00 AM 3:00 AM 6:00 AM 8:30 AM 11:00 AM 4:00 PM   1:00 AM 4:00 AM 7:00 AM 9:30 AM 12:00 PM 5:00 PM   2:00 AM 5:00 AM 8:00 AM 10:30 AM 1:00 PM 6:00 PM   3:00 AM 6:00 AM 9:00 AM 11:30 AM 2:00 PM 7:00 PM   4:00 AM 7:00 AM 10:00 AM 12:30 PM 3:00 PM 8:00 PM   5:00 AM 8:00 AM 11:00 AM 1:30 PM 4:00 PM 9:00 PM   6:00 AM 9:00 AM 12:00 PM 2:30 PM 5:00 PM 10:00 PM   7:00 AM 10:00 AM 1:00 PM 3:30 PM 6:00 PM 11:00 PM   8:00 AM 11:00 AM 2:00 PM 4:30 PM 7:00 PM 12:00 AM   9:00 AM 12:00 PM 3:00 PM 5:30 PM 8:00 PM 1:00 AM   10:00 AM 1:00 PM 4:00 PM 6:30 PM 9:00 PM 2:00 AM   11:00 AM 2:00 PM 5:00 PM 7:30 PM 10:00 PM 3:00 AM   12:00 PM 3:00 PM 6:00 PM 8:30 PM 11:00 PM 4:00 AM   1:00 PM 4:00 PM 7:00 PM 9:30 PM 12:00 AM 5:00 AM   2:00 PM 5:00 PM 8:00 PM 10:30 PM 1:00 AM 6:00 AM   3:00 PM 6:00 PM 9:00 PM 11:30 PM 2:00 AM 7:00 AM   4:00 PM 7:00 PM 10:00 PM 12:30 AM 3:00 AM 8:00 AM   5:00 PM 8:00 PM 11:00 PM 1:30 AM 4:00 AM 9:00 AM   6:00 PM 9:00 PM 12:00 AM 2:30 AM 5:00 AM 10:00 AM   7:00 PM 10:00 PM 1:00 AM 3:30 AM 6:00 AM 11:00 AM   8:00 PM 11:00 PM 2:00 AM 4:30 AM 7:00 AM 12:00 PM   9:00 PM 12:00 AM 3:00 AM 5:30 AM 8:00 AM 1:00 PM   10:00 PM 1:00 AM 4:00 AM 6:30 AM 9:00 AM 2:00 PM   11:00 PM 2:00 AM 5:00 AM 7:30 AM 10:00 AM 3:00 PM      How to Nap at Work In order to actually take a nap without going home during the day, your choices are limited:\n Find a lounge room at work, some employers offer massage chairs.  Put on your headphones and doze off, assuming you can\u0026rsquo;t get in trouble for it.   Close your eyes while at your desk for a bit.  This may be a fireable offense in some corporate cultures. Chat with your colleagues and management about doing this first.   Nap in the car.  Napping in your car at work on company property can be a fireable offense. Check first.   Drive to a nearby park and nap in your car.  In Arizona, laws are in place to prevent sleeping on public property, including parks. However, napping in the car is generally fine.   Nap using an approved space at work.  It\u0026rsquo;s unlikely that your employer has this but if they do, use it!    If your management chain is adamant about not allowing naps, don\u0026rsquo;t push it. They clearly have their reasons (regardless of yours) and your reasons just aren\u0026rsquo;t good enough. Try to be smart and respectful about this.\nSleeping Less if You Nap If you follow these guidelines and take effective midday naps, can you reduce how long you spend sleeping overall?\nThis question can lead us down the rabbit hole of polyphasic sleep, which is a topic for another day. For now, the answer to this question is: No, you should not change the amount of time you normally spend sleeping if you take a nap. Midday naps are meant to augment the circadian rhythm of a normal 7-8 hour sleep duration, not replace lost sleep time.\nHowever, with that being said\u0026hellip; a well-executed nap can reverse the hormonal impact of not getting a good night\u0026rsquo;s sleep. Just don\u0026rsquo;t do it regularly - the evidence provided throughout this article does not support the notion that it\u0026rsquo;s OK to reduce longer sleep cycles in favor of naps.\n  Summary Taking a midday nap of 10-20 minutes approximately 2.5 hours after substantial food intake (such as lunch) can yield measurable neurological, biochemical, and physical changes in the body.\nIf proper napping guidance is not followed, circadian rhythm can be interrupted, leading to an inverse result.\nThose who are seeking to optimally nap during their workday already legally can do so, but must find a reasonable place to do it without committing fireable offenses, such as in their car at a park.\nFinally, there is evidence showing that a nap can help reduce the impact of getting a poor night\u0026rsquo;s sleep on occasion, but the discussion of utilizing naps to replace a typical 7-9-hour sleep schedule is protracted for another occasion.\nOther Articles The following links are suggested for your reading, in addition to the ones directly linked in the article itself.\n Mayo Clinic: Napping: Do\u0026rsquo;s and don\u0026rsquo;ts for healthy adults Sleep.org: Sleeping During the Day WikiHow: How to sleep at work Wikipedia: Power Nap    ","permalink":"https://charlesmknox.com/health/general/napping-the-right-way/","summary":"Recently I stumbled upon a Reddit post in the ChangeMyView subreddit that suggested that it should be \u0026ldquo;the standard\u0026rdquo; for employers to allow 20-minute naps around noon.\nAs someone that values having some downtime around lunch to close my eyes and nap, this topic was worth reviewing in detail. What are the benefits of taking naps? How long should naps actually be? When should they be taken? Can an employer allow an employee to take naps?","title":"Napping: The Right Way"},{"content":" Installing Postman with Canonical\u0026rsquo;s Snap does not typically work well for me, so I prefer installing it natively. Here are the instructions for doing this on Ubuntu 20.04.\nInstallation First, download the latest release:\nwget https://dl.pstmn.io/download/latest/linux64 -O postman.tar.gz Extract the underlying archive to /opt:\nsudo tar -xzvf postman.tar.gz -C /opt Create a symbolic link (shortcut) from the extracted Postman binary to /usr/bin/Postman so that it gets added to your $PATH and you can easily run it by typing the command postman:\nsudo ln -s /opt/Postman/Postman /usr/bin/postman Add a desktop shortcut with Postman icon Last but not least, to create an application entry so that Postman shows up in your list of applications, create a postman.desktop file using this command:\ncat \u0026gt; ~/.local/share/applications/postman.desktop \u0026lt;\u0026lt; EOF [Desktop Entry] Categories=Development; Encoding=UTF-8 Exec=postman Icon=/opt/Postman/app/resources/app/assets/icon.png Name=Postman Terminal=false Type=Application EOF  Learning note: The above command uses the here document syntax to push the contents between the first EOF and second EOF to the file /home/$USER/.local/share/applications/postman.desktop.\n Here\u0026rsquo;s how it will look:\nDon\u0026rsquo;t forget to clean up the downloaded postman.tar.gz file:\nrm postman.tar.gz Done! That\u0026rsquo;s it! Check your Ubuntu Activities menu (top left corner) to verify that Postman shows up. The first time I opened the Activities menu, it did not show the entry, so I opened it one more time and it showed up the second time. I\u0026rsquo;ll assume it just hadn\u0026rsquo;t built an index of all available applications on the first try.\n  ","permalink":"https://charlesmknox.com/tech/software/linux/how-to-install-postman-on-ubuntu-20.04/","summary":"Installing Postman with Canonical\u0026rsquo;s Snap does not typically work well for me, so I prefer installing it natively. Here are the instructions for doing this on Ubuntu 20.04.\nInstallation First, download the latest release:\nwget https://dl.pstmn.io/download/latest/linux64 -O postman.tar.gz Extract the underlying archive to /opt:\nsudo tar -xzvf postman.tar.gz -C /opt Create a symbolic link (shortcut) from the extracted Postman binary to /usr/bin/Postman so that it gets added to your $PATH and you can easily run it by typing the command postman:","title":"How to Install Postman on Ubuntu 20.04"},{"content":" It\u0026rsquo;s April 24, 2020. Ubuntu 20.04 just came out. Installing Docker via the method I typically use via get.docker.com does not seem to work. Additionally, the Docker website doesn\u0026rsquo;t seem to have install instructions for Ubuntu 20.04 yet. Snap installations are recommended by Canonical, but from past experience with the Docker Snap, I prefer to stick to a native method of installation.\nIf you don\u0026rsquo;t have Ubuntu 20.04 yet, it was released on April 23, 2020. Go get it here!\nIn Ubuntu 20.04, the apt package docker.io should do the trick. Continue reading for a step-by-step breakdown.\nInstallation Install the package:\nsudo apt install docker.io Enable the systemd service for Docker:\nsudo systemctl enable --now docker Ensure your user is a member of the docker group so that you don\u0026rsquo;t have to use sudo to run Docker commands:\nsudo usermod -aG docker $USER Unfortunately, you do have to log out of your current session in order for the new docker group membership to propagate to future Bash terminal sessions. The rest of the Docker commands in this guide will include sudo to make it a little easier for you, in case you choose not to log out.\nConfirm it works Then, try running Nginx locally:\nsudo docker run --rm -it -p 8080:80 nginx:alpine Visit http://localhost:8080 in your browser and verify that it works!\nQuit out of the currently running Nginx container by pressing CTRL+C. This will also clean it up, thanks to the --rm flag we passed into the docker run command.\nInstall Docker compose Next, install Docker compose by retrieving the docker-compose release binary from GitHub - these steps are on the Docker website:\nsudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose  Note that this may be out of date; please use the instructions from the release to install Docker compose. This is just an example.\n Add executable permissions to the binary:\nsudo chmod +x /usr/local/bin/docker-compose Verify by running:\nsudo docker-compose version   apt update broken - fix As of 5/15/2020, Docker has released a formal focal release. Previously, there were only eoan packages available, which caused sudo apt update to break, and required extra steps to fix it, outlined in the collapsed section below. In order to verify a successful Docker installation, update your package index now:\nsudo apt update This should succeed with no issues. If for some reason there are still issues, consider expanding \u0026amp; reviewing the content in the Fixing Apt Failures section below.\nThere\u0026rsquo;s one caveat to this installation method currently. There is currently no apt repository source for Ubuntu 20.04, seen here:\nThis will cause sudo apt update commands to fail:\nGet:1 http://us.archive.ubuntu.com/ubuntu focal InRelease [265 kB] Ign:2 https://download.docker.com/linux/ubuntu focal InRelease Err:3 https://download.docker.com/linux/ubuntu focal Release 404 Not Found [IP: 13.33.71.38 443] Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease Get:6 http://us.archive.ubuntu.com/ubuntu focal-updates InRelease [89.1 kB] Hit:7 http://us.archive.ubuntu.com/ubuntu focal-backports InRelease Get:8 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1,760 B] Reading package lists... Done E: The repository 'https://download.docker.com/linux/ubuntu focal Release' does not have a Release file. N: Updating from such a repository can't be done securely, and is therefore disabled by default. N: See apt-secure(8) manpage for repository creation and user configuration details. To fix this, edit the file /etc/apt/sources.list.d/docker.list to include an eoan release:\n# original preserved: #deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable deb [arch=amd64] https://download.docker.com/linux/ubuntu eoan stable Then, run:\nsudo apt update Done All set! You should now be able to run Docker containers, as well as orchestrate services with Docker compose files.\n  ","permalink":"https://charlesmknox.com/tech/software/linux/how-to-install-docker-on-ubuntu-20.04/","summary":"It\u0026rsquo;s April 24, 2020. Ubuntu 20.04 just came out. Installing Docker via the method I typically use via get.docker.com does not seem to work. Additionally, the Docker website doesn\u0026rsquo;t seem to have install instructions for Ubuntu 20.04 yet. Snap installations are recommended by Canonical, but from past experience with the Docker Snap, I prefer to stick to a native method of installation.\nIf you don\u0026rsquo;t have Ubuntu 20.04 yet, it was released on April 23, 2020.","title":"How to Install Docker on Ubuntu 20.04"},{"content":" The following are my notes from the TED Talks Daily podcast, Why sleep matters now more than ever | Matt Walker.\nMatt Walker has an impressive set of credentials related to sleep science. View his website at sleepdiplomat.com.\nNotes on Sleep, the Immune System, and COVID-19 Individuals that reported getting less than 7 hours of sleep reported a 3-fold increase of getting the Rhinovirus.\nThe body requires sleep in order to produce a proper immune response to even vaccines. Studies found that participants in flu shots who had less sleep for the week leading up to the shot had a 50% drop in antibody response.\nThis means that when the COVID-19 vaccine is released, individuals must make sure they get enough sleep for a minimum of a week beforehand.\nLearning Sleep prepares the brain for learning, almost like a dry sponge ready to absorb new things. Additionally, sleep (after learning) performs something like a \u0026ldquo;file transfer\u0026rdquo; mechanism where it transfers the learnings from a short-term reservoir to a more permanent reservoir.\nSleep will actually take new memories and associate them with pre-existing stores of information, almost like sleep alchemy.\nThis leads to a 3-fold increase in creative insights (hence why you\u0026rsquo;ve been told to sleep on a tough problem).\nHunger The quality of your sleep impacts two hormones in your body that relate to your consumption and perception of food:\n Leptin - hormone that tells your brain when you\u0026rsquo;re full Ghrelin - hormone that tells your brain when you\u0026rsquo;re still hungry  When you lose sleep, these two hormones go in opposite directions. The signals from leptin are reduced, so we don\u0026rsquo;t know when we\u0026rsquo;re full, and the signals from ghrelin are increased, so we continue to eat even though we have reached a sufficient level of food intake.\nAdditionally, when sleep loss occurs, the preference for food groups shifts: You start to desire more heavy-hitting carbohydrates and more simple processed sugary foods, rather than more healthy macro-ingredients.\nTips on Enhancing Sleep Quality  Keep the room cool before bed and during the night. Keep the lights dim (personally, switching Philips Hue lights to a dim red works well). No caffeine many hours before bedtime. (not from the podcast, but I recommend 6+ hours) Manage your sleep/awake cycle consistently, regardless of if you had a full night\u0026rsquo;s sleep.  If you did not go to bed at a good hour, do not sleep in. Instead, suffer through it for the day, and wake up at the same time every day.   Avoid napping to make up for lost sleep. If you\u0026rsquo;re having trouble falling asleep for more than 30-45 minutes, do not let anxiety take over:  Get up, do something away from screens, bright lights, etc (such as reading a book under dim lighting) Do the above until you feel sleepy. This pattern creates a predictable pattern for the body to follow. Instead of associating your bed with a place of stress, only going to bed when you\u0026rsquo;re properly tired allows the body to properly maintain an effective sleep routine. Sleeping is a bit more like landing a plane than an on/off switch.   Consider removing all visible clocks from your room. Knowing the time does not help with managing stress when you\u0026rsquo;re failing to fall asleep.    Devices Before Bed We\u0026rsquo;ve probably all heard this, but using social media on your devices within a reasonable amount of time before bed and after waking up causes spikes in stress. Avoid checking notifications and social media for a minimum of 5 minutes before and after bed for starters - the longer you can avoid it, the better.\nMelatonin Melatonin (Wikipedia) can be used to help regulate the sleep cycle. However, it will not resolve other sleep issues. It is akin to a gun shot to start a sprint race - the gun is responsible for starting the race, but the participants in the race are still responsible for working through the race itself.\nRecommended Amount of Sleep 7-9 hours of sleep. A small amount of people can innately sleep 5-6 hours. You\u0026rsquo;re probably not one of them.\nPersonal Recommendation From me, not Matt Walker:\n I recommend a comfortable sleeping face mask as well. I tried a few before settling on the one linked. A fan to create white noise can help cancel out other noises as well as keep your room cool overnight. The website Sleep Like the Dead provides a unique filtering mechanism for actual mattresses. Exercise helps regulate the sleep cycle. Exercising in the morning ensures that you\u0026rsquo;re tired by the time 8pm rolls around.  Summary  Reduce sources of anxiety and blue light when it comes time to sleep. Consistent sleep cycles are critical, regardless of how late you go to bed. Sleep promotes weight management, immune response, and problem solving skills.    ","permalink":"https://charlesmknox.com/health/general/why-sleep-matters-now-more-than-ever/","summary":"The following are my notes from the TED Talks Daily podcast, Why sleep matters now more than ever | Matt Walker.\nMatt Walker has an impressive set of credentials related to sleep science. View his website at sleepdiplomat.com.\nNotes on Sleep, the Immune System, and COVID-19 Individuals that reported getting less than 7 hours of sleep reported a 3-fold increase of getting the Rhinovirus.\nThe body requires sleep in order to produce a proper immune response to even vaccines.","title":"Why Sleep Matters Now More Than Ever"},{"content":" The following Go code snippet will convert a string from this:\nWow! This message's getting some _good_ publicity! To this:\nwow-this-messages-getting-some-good The function The function itself, and its imports:\nimport ( \u0026#34;math\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;strings\u0026#34; ) // Converts a string to a lowercase, hyphen-separated string of max length 36 func getTitleURLFromString(title string) (output string) { // first, strip out any special characters  re := regexp.MustCompile(`(?m)[^\\d^A-Z^a-z^\\-^\\s]`) substitution := \u0026#34;\u0026#34; output = re.ReplaceAllString(title, substitution) // set to lowercase  output = strings.ToLower(output) // next, replace all whitespace characters with dashes  re = regexp.MustCompile(`(?m)[\\s]`) substitution = \u0026#34;-\u0026#34; output = re.ReplaceAllString(output, substitution) // replace \u0026#34;clumps\u0026#34; of 2 or more hyphens with 1 hyphen  re = regexp.MustCompile(`(?m)-{2,}`) substitution = \u0026#34;-\u0026#34; output = re.ReplaceAllString(output, substitution) // result is only up to 36 characters (or the whole thing if less than 36)  output = output[:int(math.Min(float64(len(output)), 36))] // remove trailing hyphens from the final output  re = regexp.MustCompile(`(?m)-*$`) substitution = \u0026#34;\u0026#34; output = re.ReplaceAllString(output, substitution) return output } Unit tests package main import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; ) func TestGetTitleURLFromString(t *testing.T) { tests := []struct { input string expected string }{ { \u0026#34;\u0026#39;\u0026#39;\u0026#39;@@)(*F)(*)(*#)(@)(*@#$%\u0026amp;)(@#*$\u0026#34;, \u0026#34;f\u0026#34;, }, { \u0026#34;ugly text here!!!??\u0026#34;, \u0026#34;ugly-text-here\u0026#34;, }, { \u0026#34;hyphenated-but - still-good!\u0026#34;, \u0026#34;hyphenated-but-still-good\u0026#34;, }, { \u0026#34;hyphen at the end should go away and this should be 36 characters)(SD\u0026amp;*-\u0026#34;, \u0026#34;hyphen-at-the-end-should-go-away-and\u0026#34;, }, { \u0026#34;Wow! This message\u0026#39;s getting some _good_ publicity!\u0026#34;, \u0026#34;wow-this-messages-getting-some-good\u0026#34;, }, } for _, test := range tests { actual := getTitleURLFromString(test.input) assert.Equal(t, test.expected, actual) } } Final note Go Playground Link\nAdditionally, regex101.com is a great resource for interactively testing regular expressions, and it includes a Golang (plus other languages) source code generator to make this process easy.\n  ","permalink":"https://charlesmknox.com/tech/software/go/regular-expression-for-url-friendly-titles/","summary":"The following Go code snippet will convert a string from this:\nWow! This message's getting some _good_ publicity! To this:\nwow-this-messages-getting-some-good The function The function itself, and its imports:\nimport ( \u0026#34;math\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;strings\u0026#34; ) // Converts a string to a lowercase, hyphen-separated string of max length 36 func getTitleURLFromString(title string) (output string) { // first, strip out any special characters  re := regexp.MustCompile(`(?m)[^\\d^A-Z^a-z^\\-^\\s]`) substitution := \u0026#34;\u0026#34; output = re.","title":"Regular Expression for URL-Friendly Titles"},{"content":" Here\u0026rsquo;s how to compress and decompress a string in Golang using the gzip library.\npackage main import ( \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;bytes\u0026#34; \u0026#34;io/ioutil\u0026#34; ) // Credit where credit is due! // https://stackoverflow.com/a/19267224 func compressString(str string) (string, error) { var b bytes.Buffer gz := gzip.NewWriter(\u0026amp;b) if _, err := gz.Write([]byte(str)); err != nil { fmt.Println(fmt.Sprintf(\u0026#34;Compress string failure %v\u0026#34;, err)) return \u0026#34;\u0026#34;, err } if err := gz.Close(); err != nil { fmt.Println(fmt.Sprintf(\u0026#34;Compress string closure failure %v\u0026#34;, err)) return \u0026#34;\u0026#34;, err } return string(b.Bytes()), nil } func decompressString(str string) (string, error) { gr, err := gzip.NewReader(bytes.NewBuffer([]byte(str))) defer gr.Close() data, err := ioutil.ReadAll(gr) if err != nil { return \u0026#34;\u0026#34;, err } return string(data), nil } Go Playground Link\n  ","permalink":"https://charlesmknox.com/tech/software/go/string-compression/","summary":"Here\u0026rsquo;s how to compress and decompress a string in Golang using the gzip library.\npackage main import ( \u0026#34;compress/gzip\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;bytes\u0026#34; \u0026#34;io/ioutil\u0026#34; ) // Credit where credit is due! // https://stackoverflow.com/a/19267224 func compressString(str string) (string, error) { var b bytes.Buffer gz := gzip.NewWriter(\u0026amp;b) if _, err := gz.Write([]byte(str)); err != nil { fmt.Println(fmt.Sprintf(\u0026#34;Compress string failure %v\u0026#34;, err)) return \u0026#34;\u0026#34;, err } if err := gz.Close(); err != nil { fmt.","title":"String Compression in Go"},{"content":" By default, Angular does not work very well with Google Analytics. It will only announce a page-view event when refreshing the page. This makes the router in Angular somewhat useless.\nLuckily, there is an npm package called angular-gtag that hooks into the root app component of your Angular project and sends views upon router navigation.\nHowever, I found that the README had a few implicit steps that I needed to do some searching for. To save others time, I\u0026rsquo;ve compiled my steps into this article.\nGetting Started Install and configure the package according to the README.md for angular-gtag on GitHub. Ensure that the steps you take match the steps outlined here as well as the ones on the README.\nIn app.module.ts, add the import at the top, and include it in the imports section:\n// ... other imports import { GtagModule } from \u0026#39;angular-gtag\u0026#39;; @NgModule({ declarations: [ AppComponent // other declarations  ], imports: [ BrowserModule, AppRoutingModule, HttpClientModule, GtagModule.forRoot({ trackingId: \u0026#39;UA-100102030-1\u0026#39;, trackPageviews: true }) // other imports  ], providers: [], bootstrap: [AppComponent] }) export class AppModule { } In app.component.ts, add the import at the top for Gtag, and add it to the constructor:\nimport { Component } from \u0026#39;@angular/core\u0026#39;; import { DataService } from \u0026#39;./data.service\u0026#39;; import { Gtag } from \u0026#39;angular-gtag\u0026#39;; @Component({ selector: \u0026#39;app-root\u0026#39;, templateUrl: \u0026#39;./app.component.html\u0026#39;, styleUrls: [\u0026#39;./app.component.scss\u0026#39;], }) export class AppComponent { constructor(public data: DataService, gtag: Gtag) { this.data.getArticles(); this.data.getArticleList(); this.data.getCategories(); } // ... other things Finally, modify the script for Google Analytics to ignore running the site from localhost so that the package works properly locally - edit the \u0026lt;head\u0026gt; section of index.html:\n\u0026lt;script\u0026gt; var host = window.location.hostname; // add this  if (host !== \u0026#34;localhost\u0026#34;) { // add this  window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;UA-100102030-1\u0026#39;, { \u0026#39;send_page_view\u0026#39;: false }); // added the \u0026#39;send_page_view\u0026#39; parameter as instructed by angular-gtag\u0026#39;s readme  } else { // add this  function gtag() {} // add this  } // add this \u0026lt;/script\u0026gt; Credit for some of these methods goes to a few users in this GitHub issue thread.\nValidate Build your site and validate that it works using Google Analytics' realtime interface.\n  ","permalink":"https://charlesmknox.com/tech/software/angular/using-angular-and-google-analytics/","summary":"By default, Angular does not work very well with Google Analytics. It will only announce a page-view event when refreshing the page. This makes the router in Angular somewhat useless.\nLuckily, there is an npm package called angular-gtag that hooks into the root app component of your Angular project and sends views upon router navigation.\nHowever, I found that the README had a few implicit steps that I needed to do some searching for.","title":"Using Angular and Google Analytics"},{"content":" By default, when using Angular routerLink tags in your HTML, the behavior users will experience when clicking these links is that the browser keeps the current scrollbar positions, before and after clicking.\nThere are cases where this is undesirable, such as when using an Angular router to route URLs (such as /home or /articles ) to Angular components.\nTo fix this, add in your app-routing.module.ts file the following into the @NgModule imports:\n@NgModule({ imports: [ RouterModule.forRoot( routes, { scrollPositionRestoration: \u0026#39;enabled\u0026#39; // https://stackoverflow.com/a/44372167  } ) ], exports: [RouterModule] }) Credit where credit is due - this was found and validated by me, but added to Stack Overflow here.\nIf you don\u0026rsquo;t have an app-routing.module.ts, well, the file (and other Angular routing integration) is auto-generated by Angular. I\u0026rsquo;ve done a bit of searching on how to add routing to an existing project that didn\u0026rsquo;t have it already, and in my opinion, it\u0026rsquo;s potentially easier (YMMV) to just generate a new project and answer \u0026lsquo;Yes\u0026rsquo; to adding routes when prompted, and migrate your services/components/etc.\n  ","permalink":"https://charlesmknox.com/tech/software/angular/how-to-fix-angular-not-scrolling-to-top-for-routerlinks/","summary":"By default, when using Angular routerLink tags in your HTML, the behavior users will experience when clicking these links is that the browser keeps the current scrollbar positions, before and after clicking.\nThere are cases where this is undesirable, such as when using an Angular router to route URLs (such as /home or /articles ) to Angular components.\nTo fix this, add in your app-routing.module.ts file the following into the @NgModule imports:","title":"How to Fix Angular Not Scrolling to Top for Routerlinks"},{"content":" The title is a bit of an exaggeration, but it\u0026rsquo;s partly true if you don\u0026rsquo;t use the correct properties in your HTML elements.\nIn HTML, you should be using rel=\u0026quot;noopener noreferrer\u0026quot; along with target=\u0026quot;_blank\u0026quot; in your \u0026lt;a\u0026gt; tags - if not, you run major security risks. This GitHub Pages site shows you how: https://apal21.github.io/target-blank/\nMany linters will catch this if it\u0026rsquo;s not set, so make sure that you\u0026rsquo;re using a linter when you write your code.\n ","permalink":"https://charlesmknox.com/tech/software/webdev/every-outbound-link-leaks-privacy/","summary":"The title is a bit of an exaggeration, but it\u0026rsquo;s partly true if you don\u0026rsquo;t use the correct properties in your HTML elements.\nIn HTML, you should be using rel=\u0026quot;noopener noreferrer\u0026quot; along with target=\u0026quot;_blank\u0026quot; in your \u0026lt;a\u0026gt; tags - if not, you run major security risks. This GitHub Pages site shows you how: https://apal21.github.io/target-blank/\nMany linters will catch this if it\u0026rsquo;s not set, so make sure that you\u0026rsquo;re using a linter when you write your code.","title":"Every Outbound Link Leaks Privacy"},{"content":" This post contains code that will enable you to automatically backup a directory and remove backups older than 3 weeks.\nThe following Bash script is a cronjob directory backup script that takes two arguments:\n the folder to backup the output directory  It will also automatically clean up any backups that are more than 3 weeks old.\nSee below.\nThe backup script #!/bin/bash -e  outputtargz=$(basename \u0026#34;${2}\u0026#34; .tar.gz) tgtpath=$(dirname \u0026#34;${2}\u0026#34;) output=\u0026#34;${tgtpath}/${outputtargz}__$(date +%m-%d-%y_%H-%M).tar.gz\u0026#34; mkdir -p \u0026#34;${tgtpath}\u0026#34; tar -czvf \u0026#34;${output}\u0026#34; \u0026#34;${1}\u0026#34; # Clean up any old files (3 weeks or older) find \u0026#34;${tgtpath}/*\u0026#34; -mtime +21 -type f -delete echo \u0026#34;Backed up ${1}to ${output}successfully\u0026#34; Usage It can be used like this:\n./backup_script.sh /path/to/backup /target/location   Cronjob: Automated backups It can also be used as a cronjob. To edit your crontab, use the command:\ncrontab -e You may need to run the crontab -e command with sudo if your backup source or target is not readable by the current user.\nHere\u0026rsquo;s an example cronjob that runs every 6 hours at 12am, 6am, 12pm, and 6pm:\n# Install this as a cronjob: SHELL=/bin/bash 0 0,6,12,18 * * * /path/to/this/backup_script.sh /home/my/folder1 /mnt/backups/folder1.tar.gz Final note For help with crontab job intervals, use the handy website crontab.guru.\nThis can be used on Ubuntu, Linux Mint, Arch Linux, or any other Linux OS/environment that supports running bash and shell scripts natively.\n  ","permalink":"https://charlesmknox.com/tech/software/bash/backup-a-folder-in-bash/","summary":"This post contains code that will enable you to automatically backup a directory and remove backups older than 3 weeks.\nThe following Bash script is a cronjob directory backup script that takes two arguments:\n the folder to backup the output directory  It will also automatically clean up any backups that are more than 3 weeks old.\nSee below.\nThe backup script #!/bin/bash -e  outputtargz=$(basename \u0026#34;${2}\u0026#34; .tar.gz) tgtpath=$(dirname \u0026#34;${2}\u0026#34;) output=\u0026#34;${tgtpath}/${outputtargz}__$(date +%m-%d-%y_%H-%M).","title":"How to Backup a Directory/Folder in Bash"},{"content":" Swagger is a framework to enable arbitrary implementations of the OpenAPI specification.\nWhy? In all of the projects I\u0026rsquo;ve worked on (personal and professional), one of the more consistent problems is documentation of API endpoints, as well as agreement upon REST interaction schema between microservices. It wasn\u0026rsquo;t until a few years into my software development career that I became aware of Swagger.\nSwagger lets you:\n auto-generate interactive documentation for your REST API auto-generate a client to consume your defined REST API in many programming languages/frameworks auto-generate a server to host your defined REST API in many programming languages/frameworks  For the bright software developer that wishes to move fast but still produce quality software, Swagger is an optimal choice for delivering results.\nPreview of the Swagger UI Take a look for yourself:\nGetting Started Clone the Swagger repository:\ngit clone https://github.com/swagger-api/swagger-editor.git Run it locally by running the following command - this may take a while:\nnpm start Navigate to http://localhost:3001 in your browser.\nI personally always recommend using the Docker deployment methods where possible. Initially, I chose not to use Docker and instead went with native Node because I was worried about mounting files between the Swagger container and my host. However, after using it, I found that I do not use Swagger for writing files, I just copy/paste between my browser. So, feel free to just run the Dockerized version.\nStart Minimal The Swagger editor comes with a fully populated example. Sift through each of the items to build out a desirable schema for your needs.\nopenapi: 3.0.1 info: title: Article Server description: \u0026#39;Article server\u0026#39; version: 1.0.0 servers: - url: http://localhost:8080/api - url: https://charlesmknox.com/api/articles paths: /articles: TBD components: schemas: Article: TBD Articles: TBD This article will be covering the TBD sections above.\nCreate the Article JSON Object Using Swagger Toolbox, a basic JSON structure can immediately be converted to a Swagger schema definition.\nHere\u0026rsquo;s the basic structure for an Article data type:\n{ \u0026#34;title\u0026#34;: \u0026#34;An Article\u0026#34;, \u0026#34;articleID\u0026#34;: \u0026#34;9e4c8443-c89f-4c1d-8b03-fb25e327780c\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2020-04-15T15:38:10.243Z\u0026#34;, \u0026#34;authorID\u0026#34;: \u0026#34;5af053ca-1bfe-46ed-8578-f6f420a44bb0\u0026#34;, \u0026#34;markdown\u0026#34;: \u0026#34;Here\u0026#39;s some `markdown content`.\u0026#34;, \u0026#34;renderedMarkdown\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;categoryIDs\u0026#34;: [ \u0026#34;d6d04c55-3645-49f3-a512-818f5fc127ff\u0026#34; ] } Ensure there are no syntax errors in the input JSON. The error messages from the toolbox may not directly indicate that there are syntax errors.\nHere is the output yaml content:\n--- required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34; Note: Part of yaml syntax includes --- as a header. This will be removed when adding this schema to the final Swagger yaml document.\nImproving the Article Schema In order to get a more effective schema defined, constraints can be applied to some data types, such as the one associated with the date field.\nFor comprehensive reference, please refer to the Swagger docs: https://swagger.io/docs/specification/data-models\nFor the OpenAPI specification on data types, visit this page: https://swagger.io/specification/#dataTypes\nIn particular, update the date property from this:\ndate: type: \u0026#34;string\u0026#34; To this:\ndate: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; The date-time format will expect values to follow the ISO8601 standard date format - example: 2020-04-15T15:38:10.243Z.\nThe result is now:\n--- required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34;  Completing the Article Schema The things done up to this point still have not actually completed the full schema definition, only the data structure within the schema. In order to complete the schema, tack-on a few values surrounding the structure from above:\nArticle: type: object required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34; The components section of the Swagger yaml document should be modified to include this now:\ncomponents: schemas: Article: type: object required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34; Create an Array of Articles Now that an Article schema has been defined, it will become necessary to return an array of Article objects as part of a typical API response. To do this, Swagger offers the $ref keyword, which points to any item in the Swagger yml document that\u0026rsquo;s been defined elsewhere. This feature will be used to set up a new Articles schema to simply be an array of Article objects.\nFor documentation on the $ref keyword, visit this page: https://swagger.io/docs/specification/using-ref/\nHere is an example of the new Articles schema:\nArticles: type: array items: $ref: \u0026#39;#/components/schemas/Article\u0026#39; Add this to the components section in the Swagger yaml file:\ncomponents: schemas: Article: type: object required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34; Articles: type: array items: $ref: \u0026#39;#/components/schemas/Article\u0026#39; Create Articles REST API Endpoint Now that the schema has been properly defined, the next step is to create a REST API endpoint to return Articles objects.\nIn the paths section of the Swagger yaml file, a new endpoint will be created. This section is easy, since the schemas have been created already:\npaths: /articles: get: summary: Retrieve all articles responses: 200: description: successful operation content: application/json: schema: type: array items: $ref: \u0026#39;#/components/schemas/Articles\u0026#39; In this API endpoint definition, it\u0026rsquo;s clear that only HTTP 200 OK responses are expected and supported. Endpoint definitions can include other HTTP status code responses, such as 400 Bad Request, 500 Internal Server Error, etc.\nThe Final Swagger Document The final document will look like this now:\nopenapi: 3.0.1 info: title: Article Server description: \u0026#39;Article server\u0026#39; version: 1.0.0 servers: - url: http://localhost:8080/api - url: https://charlesmknox.com/blog/api paths: /articles: get: summary: Retrieve all articles responses: 200: description: successful operation content: application/json: schema: type: array items: $ref: \u0026#39;#/components/schemas/Articles\u0026#39; components: schemas: Article: type: object required: - \u0026#34;title\u0026#34; - \u0026#34;articleID\u0026#34; - \u0026#34;date\u0026#34; - \u0026#34;authorID\u0026#34; - \u0026#34;markdown\u0026#34; - \u0026#34;renderedMarkdown\u0026#34; - \u0026#34;categoryIDs\u0026#34; properties: title: type: \u0026#34;string\u0026#34; articleID: type: \u0026#34;string\u0026#34; date: type: \u0026#34;string\u0026#34; format: \u0026#34;date-time\u0026#34; authorID: type: \u0026#34;string\u0026#34; markdown: type: \u0026#34;string\u0026#34; renderedMarkdown: type: \u0026#34;string\u0026#34; categoryIDs: type: \u0026#34;array\u0026#34; items: type: \u0026#34;string\u0026#34; Articles: type: array items: $ref: \u0026#39;#/components/schemas/Article\u0026#39;   Generate Server \u0026amp; Documentation Click \u0026ldquo;Generate Server\u0026rdquo; at the top and choose a server to run, such as go-server or nodejs-server. Since the beginning of this guide uses Node, the nodejs-server will be chosen.\nWhen running the server, documentation is hosted at http:/localhost:8080/docs.\nClick nodejs-server. The browser will download a zip archive containing source code. Extract the files into a directory, example Bash steps:\nmkdir swagger-node-article-server \u0026amp;\u0026amp; \\ mv nodejs-server-server-generated.zip swagger-node-article-server \u0026amp;\u0026amp; \\ cd swagger-node-article-server \u0026amp;\u0026amp; \\ unzip nodejs-server-server-generated.zip The directory looks like this:\nls -alht total 84K -rw-r--r-- 1 chuck chuck 698 Apr 15 2020 index.js -rw-r--r-- 1 chuck chuck 570 Apr 15 2020 README.md -rw-r--r-- 1 chuck chuck 1.1K Apr 15 2020 .swagger-codegen-ignore drwxr-xr-x 8 chuck chuck 4.0K Apr 15 10:42 . -rw-r--r-- 1 chuck chuck 32K Apr 15 10:42 package-lock.json -rw-r--r-- 1 chuck chuck 372 Apr 15 10:42 package.json drwxr-xr-x 101 chuck chuck 4.0K Apr 15 10:42 node_modules drwxr-xr-x 2 chuck chuck 4.0K Apr 15 10:41 api drwxr-xr-x 2 chuck chuck 4.0K Apr 15 10:41 controllers drwxr-xr-x 2 chuck chuck 4.0K Apr 15 10:41 service drwxr-xr-x 2 chuck chuck 4.0K Apr 15 10:41 .swagger-codegen drwxr-xr-x 2 chuck chuck 4.0K Apr 15 10:41 utils drwxr-xr-x 14 chuck chuck 4.0K Apr 15 10:41 .. -rw-rw-r-- 1 chuck chuck 3.9K Apr 15 10:38 nodejs-server-server-generated.zip Run npm install:\nnpm install Run npm start:\nnpm start This will start the server on http://localhost:8080/ - leave this running in the background.\nGenerate a Client to Consume the Server Return to the original Swagger editor page (at http://localhost:3001) and click \u0026ldquo;Generate Client\u0026rdquo;. In that menu, click \u0026ldquo;html2\u0026rdquo;, and in a manner similar to the \u0026ldquo;Generate Server\u0026rdquo; functionality, a zip file will be downloaded. Proceed to extract the files from the zip file like before:\nmkdir swagger-node-article-client \u0026amp;\u0026amp; \\ mv html2-client-generated.zip swagger-node-article-client \u0026amp;\u0026amp; \\ cd swagger-node-article-client \u0026amp;\u0026amp; \\ unzip html2-client-generated.zip Open the index.html in your browser:\nfirefox index.html Play around with the web page and observe that it provides documentation and instructions on how to consume the API using various clients.\nRelated Content  https://openapi.tools/ - a website containing a list of tools related to OpenAPI documents.  Summary In this article, the process of using the OpenAPI specification and Swagger to create an API with data structures, documentation, client, and server code has been demonstrated.\nIn your future software development projects, consider starting with Swagger, and maintaining your Swagger yaml document(s) throughout the lifecycle of your project.\nStay healthy!\nChuck\n  ","permalink":"https://charlesmknox.com/tech/software/openapi/swagger-and-openapi/","summary":"Swagger is a framework to enable arbitrary implementations of the OpenAPI specification.\nWhy? In all of the projects I\u0026rsquo;ve worked on (personal and professional), one of the more consistent problems is documentation of API endpoints, as well as agreement upon REST interaction schema between microservices. It wasn\u0026rsquo;t until a few years into my software development career that I became aware of Swagger.\nSwagger lets you:\n auto-generate interactive documentation for your REST API auto-generate a client to consume your defined REST API in many programming languages/frameworks auto-generate a server to host your defined REST API in many programming languages/frameworks  For the bright software developer that wishes to move fast but still produce quality software, Swagger is an optimal choice for delivering results.","title":"Swagger and OpenAPI"},{"content":" It was a bit annoying, but the regular expression on this site almost worked out of the box. I had to add in a few line breaks:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) var emailRegEx = regexp.MustCompile( `^(?:[a-z0-9!#$%\u0026amp;\u0026#39;*+/=?^_`+ \u0026#34;`\u0026#34;+ `{|}~-]+(?:\\.[a-z0-9!#$%\u0026amp;\u0026#39;*+/=?^_`+ \u0026#34;`\u0026#34;+ `{|}~-]+)*|\u0026#34;(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\u0026#34;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])`) func verifyEmail(email string) bool { if (emailRegEx.MatchString(email) == true) { return true } return false } func main() { fmt.Println(verifyEmail(\u0026#34;some.user@gmail.com\u0026#34;)) } Go Playground link: https://play.golang.org/p/Gq67bAJeK2r\n  ","permalink":"https://charlesmknox.com/tech/software/go/email-regexp/","summary":" It was a bit annoying, but the regular expression on this site almost worked out of the box. I had to add in a few line breaks:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) var emailRegEx = regexp.MustCompile( `^(?:[a-z0-9!#$%\u0026amp;\u0026#39;*+/=?^_`+ \u0026#34;`\u0026#34;+ `{|}~-]+(?:\\.[a-z0-9!#$%\u0026amp;\u0026#39;*+/=?^_`+ \u0026#34;`\u0026#34;+ `{|}~-]+)*|\u0026#34;(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\u0026#34;)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])`) func verifyEmail(email string) bool { if (emailRegEx.MatchString(email) == true) { return true } return false } func main() { fmt.Println(verifyEmail(\u0026#34;some.user@gmail.com\u0026#34;)) } Go Playground link: https://play.golang.org/p/Gq67bAJeK2r\n  ","title":"Go Regular Expression for Emails"},{"content":" Disclaimer: I\u0026rsquo;m using Go version 1.12.7 for this. It may not work for earlier versions of Go.\nIf you\u0026rsquo;ve ever tried to get into Go, one of the first things you\u0026rsquo;ve probably struggled with is simply creating a Go module. Go likes to nag you about $GOPATH, and as someone that is not a native Gopher, I don\u0026rsquo;t want to deal with it.\nEver run into this error when running go mod init? I certainly have.\n$ go mod init go: cannot determine module path for source directory /some/path/... (outside GOPATH, no import comments) In order to get past this, you can simply run this:\n$ go mod init `pwd` go: creating new go.mod: module /some/path/... Credit goes to joshuamkite on GitHub here.\n  ","permalink":"https://charlesmknox.com/tech/software/go/go-mod-init/","summary":"Disclaimer: I\u0026rsquo;m using Go version 1.12.7 for this. It may not work for earlier versions of Go.\nIf you\u0026rsquo;ve ever tried to get into Go, one of the first things you\u0026rsquo;ve probably struggled with is simply creating a Go module. Go likes to nag you about $GOPATH, and as someone that is not a native Gopher, I don\u0026rsquo;t want to deal with it.\nEver run into this error when running go mod init?","title":"go mod init Not Working"}]